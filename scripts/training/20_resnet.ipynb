{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"20_resnet.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1RKWuSAmkrrfEVkV4ByHyhAYi_JIdBoV4","authorship_tag":"ABX9TyNG2UCbDl9JtNlf8WO9lX+I"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lOzwv5AssLuz"},"source":["### Generator に ResNet を試してみる\n","Reference: https://machinelearningmastery.com/how-to-develop-cyclegan-models-from-scratch-with-keras/"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4u8Wf5j_r8qr","colab":{"base_uri":"https://localhost:8080/","height":350},"executionInfo":{"status":"ok","timestamp":1599190055295,"user_tz":-540,"elapsed":1216,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}},"outputId":"7d37b3e4-2855-4889-cf79-169e67b4a32b"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Fri Sep  4 03:27:34 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nDuVbZjksTOy","colab":{},"executionInfo":{"status":"ok","timestamp":1599192533027,"user_tz":-540,"elapsed":687,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["from tensorflow_addons.layers import InstanceNormalization\n","from tensorflow.keras.layers import Input, Dropout, Concatenate\n","from tensorflow.keras.layers import LeakyReLU, Activation\n","from tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.initializers import RandomNormal"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mNa37SULsXm-","colab":{},"executionInfo":{"status":"ok","timestamp":1599190060655,"user_tz":-540,"elapsed":483,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import datetime\n","import os"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"knAJGu-csZ-g","colab":{},"executionInfo":{"status":"ok","timestamp":1599190073763,"user_tz":-540,"elapsed":1146,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["from glob import glob\n","import cv2\n","from matplotlib.pyplot import imread"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fIdTxQKPsb0r","colab":{},"executionInfo":{"status":"ok","timestamp":1599190079293,"user_tz":-540,"elapsed":886,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["DATA_DIR_PATH = '/content/drive/My Drive/kikagaku/novelgan/data'\n","\n","OUTPUT_DIR_PATH = os.path.join(DATA_DIR_PATH, '20_out')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Z4aFC8oHsgwu","colab":{},"executionInfo":{"status":"ok","timestamp":1599190081739,"user_tz":-540,"elapsed":1741,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["#os.makedirs(os.path.join(DATA_DIR_PATH, 'datasets'), exist_ok=True)\n","\n","os.makedirs(os.path.join(OUTPUT_DIR_PATH, 'images'), exist_ok=True)\n","os.makedirs(os.path.join(OUTPUT_DIR_PATH, 'saved_models'), exist_ok=True)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ppAnPzoIsl6z","colab":{},"executionInfo":{"status":"ok","timestamp":1599190083811,"user_tz":-540,"elapsed":764,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["class DataLoader():\n","    def __init__(self, dataset_name, img_res=(256, 256)):\n","        self.dataset_name = dataset_name\n","        self.img_res = img_res\n","        self.datasets_path = os.path.join(DATA_DIR_PATH, 'datasets')\n","\n","    def load_data(self, domain, batch_size=1, is_testing=False):\n","        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n","        #path = glob('./datasets/%s/%s/*' % (self.dataset_name, data_type))\n","        path = glob(os.path.join(self.datasets_path, self.dataset_name, data_type, '*'))\n","\n","        batch_images = np.random.choice(path, size=batch_size)\n","\n","        imgs = []\n","        for img_path in batch_images:\n","            img = self.imread(img_path)\n","            if not is_testing:\n","                #img = scipy.misc.imresize(img, self.img_res)\n","                img = cv2.resize(img, self.img_res)\n","\n","                if np.random.random() > 0.5:\n","                    img = np.fliplr(img)\n","            else:\n","                #img = scipy.misc.imresize(img, self.img_res)\n","                img = cv2.resize(img, self.img_res)\n","            imgs.append(img)\n","\n","        imgs = np.array(imgs)/127.5 - 1.\n","\n","        return imgs\n","\n","    def load_batch(self, batch_size=1, is_testing=False):\n","        data_type = \"train\" if not is_testing else \"val\"\n","        #path_A = glob('./datasets/%s/%sA/*' % (self.dataset_name, data_type))\n","        #path_B = glob('./datasets/%s/%sB/*' % (self.dataset_name, data_type))\n","        path_A = glob(os.path.join(self.datasets_path, self.dataset_name, '{}A'.format(data_type), '*'))\n","        path_B = glob(os.path.join(self.datasets_path, self.dataset_name, '{}B'.format(data_type), '*'))\n","\n","        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n","        total_samples = self.n_batches * batch_size\n","\n","        # Sample n_batches * batch_size from each path list so that model sees all\n","        # samples from both domains\n","        path_A = np.random.choice(path_A, total_samples, replace=False)\n","        path_B = np.random.choice(path_B, total_samples, replace=False)\n","\n","        #for i in range(self.n_batches-1):\n","        for i in range(self.n_batches):\n","            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n","            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n","            imgs_A, imgs_B = [], []\n","\n","            for img_A, img_B in zip(batch_A, batch_B):\n","                img_A = self.imread(img_A)\n","                img_B = self.imread(img_B)\n","\n","                #img_A = scipy.misc.imresize(img_A, self.img_res)\n","                img_A = cv2.resize(img_A, self.img_res)\n","                #img_B = scipy.misc.imresize(img_B, self.img_res)\n","                img_B = cv2.resize(img_B, self.img_res)\n","\n","                if not is_testing and np.random.random() > 0.5:\n","                    img_A = np.fliplr(img_A)\n","                    img_B = np.fliplr(img_B)\n","\n","                imgs_A.append(img_A)\n","                imgs_B.append(img_B)\n","\n","            imgs_A = np.array(imgs_A)/127.5 - 1.\n","            imgs_B = np.array(imgs_B)/127.5 - 1.\n","\n","            yield imgs_A, imgs_B\n","\n","    def load_img(self, path):\n","        img = self.imread(path)\n","        #img = scipy.misc.imresize(img, self.img_res)\n","        img = cv2.resize(img, self.img_res)\n","        img = img/127.5 - 1.\n","        return img[np.newaxis, :, :, :]\n","\n","    def imread(self, path):\n","        #return scipy.misc.imread(path, mode='RGB').astype(np.float)\n","        return imread(path).astype(np.float)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x86W1mhwspDR","colab":{},"executionInfo":{"status":"ok","timestamp":1599190085839,"user_tz":-540,"elapsed":746,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["IMG_ROWS = 256\n","IMG_COLS = 256\n","\n","data_loader = DataLoader('vangogh2novel', img_res=(IMG_ROWS, IMG_COLS))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SvahgPums0Zr","colab":{},"executionInfo":{"status":"ok","timestamp":1599195735885,"user_tz":-540,"elapsed":1650,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["class CycleGAN:\n","    def __init__(self):\n","        self.history = pd.DataFrame({}, columns=[\n","            'epoch', 'epochs', 'batch_idx', 'batch_num', 'd_loss', 'acc', 'g_loss',\n","            'adv', 'recon', 'id', 'elapsed_time'])\n","\n","        self.img_save_dir = os.path.join(OUTPUT_DIR_PATH, 'images')\n","        self.model_save_dir = os.path.join(OUTPUT_DIR_PATH, 'saved_models')\n","        self.combined_name = 'combined_model'\n","        self.g_AB_name = 'g_AB_model'\n","        self.g_BA_name = 'g_BA_model'\n","\n","        self.train_cnt = 0\n","\n","        # Input shape\n","        self.img_rows = IMG_ROWS\n","        self.img_cols = IMG_COLS\n","        self.channels = 3\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","\n","        self.d_A = None\n","        self.d_B = None\n","        self.g_AB = None\n","        self.g_BA = None\n","        self.combined = None\n","\n","    def init(self, data_loader=None):\n","        # Configure data loader\n","        #self.dataset_name = 'apple2orange'\n","        #self.data_loader = DataLoader(dataset_name=self.dataset_name,\n","        #                              img_res=(self.img_rows, self.img_cols))\n","        if data_loader:\n","            self.data_loader = data_loader\n","            self.dataset_name = self.data_loader.dataset_name \n","\n","        # Calculate output shape of D (PatchGAN)\n","        patch = int(self.img_rows / 2**4)\n","        self.disc_patch = (patch, patch, 1)\n","\n","        # Number of filters in the first layer of G and D\n","        #self.gf = 32 # U-Net, 128\n","        self.gf = 64\n","        self.df = 64\n","\n","        # Loss weights\n","        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n","        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n","\n","        optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminators\n","        self.d_A = self.build_discriminator()\n","        self.d_B = self.build_discriminator()\n","        self.d_A.compile(loss='mse',\n","                         optimizer=optimizer,\n","                         metrics=['accuracy'])\n","        self.d_B.compile(loss='mse',\n","                         optimizer=optimizer,\n","                         metrics=['accuracy'])\n","\n","        # Build the generators\n","        self.g_AB = self.build_generator()\n","        self.g_BA = self.build_generator()\n","\n","        # Input images from both domains\n","        img_A = Input(shape=self.img_shape)\n","        img_B = Input(shape=self.img_shape)\n","\n","        # Translate images to the other domain\n","        fake_B = self.g_AB(img_A)\n","        fake_A = self.g_BA(img_B)\n","\n","        # Translate images back to original domain\n","        reconstr_A = self.g_BA(fake_B)\n","        reconstr_B = self.g_AB(fake_A)\n","\n","        # Identity mapping of images\n","        img_A_id = self.g_BA(img_A)\n","        img_B_id = self.g_AB(img_B)\n","\n","        # For the combined model we will only train the generators\n","        self.d_A.trainable = False\n","        self.d_B.trainable = False\n","\n","        # Discriminators determines validity of translated images\n","        valid_A = self.d_A(fake_A)\n","        valid_B = self.d_B(fake_B)\n","\n","        # Combined model trains generators to fool discriminators\n","        self.combined = Model(inputs=[img_A, img_B],\n","                              outputs=[valid_A, valid_B,\n","                                       reconstr_A, reconstr_B,\n","                                       img_A_id, img_B_id ])\n","        self.combined.compile(loss=['mse', 'mse',\n","                                    'mae', 'mae',\n","                                    'mae', 'mae'],\n","                              loss_weights=[1, 1,\n","                                            self.lambda_cycle, self.lambda_cycle,\n","                                            self.lambda_id, self.lambda_id ],\n","                              optimizer=optimizer)\n","\n","    #def define_generator(image_shape=(256,256,3), n_resnet=9):\n","    def build_generator(self, n_resnet=9):\n","        # \"\"\"U-Net Generator\"\"\"\n","\n","        # def conv2d(layer_input, filters, f_size=4):\n","        #     \"\"\"Layers used during downsampling\"\"\"\n","        #     d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n","        #     d = LeakyReLU(alpha=0.2)(d)\n","        #     d = InstanceNormalization()(d)\n","        #     return d\n","\n","        # def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n","        #     \"\"\"Layers used during upsampling\"\"\"\n","        #     u = UpSampling2D(size=2)(layer_input)\n","        #     u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n","        #     if dropout_rate:\n","        #         u = Dropout(dropout_rate)(u)\n","        #     u = InstanceNormalization()(u)\n","        #     u = Concatenate()([u, skip_input])\n","        #     return u\n","\n","        # # Image input\n","        # d0 = Input(shape=self.img_shape)\n","\n","        # # U-Net, 12\n","        # ## Downsampling\n","        # #d1 = conv2d(d0, self.gf)\n","        # #d2 = conv2d(d1, self.gf*2)\n","        # #d3 = conv2d(d2, self.gf*4)\n","        # #d4 = conv2d(d3, self.gf*8)\n","        # #\n","        # ## Upsampling\n","        # #u1 = deconv2d(d4, d3, self.gf*4)\n","        # #u2 = deconv2d(u1, d2, self.gf*2)\n","        # #u3 = deconv2d(u2, d1, self.gf)\n","        # #\n","        # #u4 = UpSampling2D(size=2)(u3)\n","        # #output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n","\n","        # # Downsampling\n","        # d1 = conv2d(d0, self.gf)\n","        # d2 = conv2d(d1, self.gf*2)\n","        # d3 = conv2d(d2, self.gf*4)\n","        # d4 = conv2d(d3, self.gf*8)\n","        # d5 = conv2d(d4, self.gf*8)\n","        # d6 = conv2d(d5, self.gf*8)\n","        # d7 = conv2d(d6, self.gf*8)\n","\n","        # # Upsampling\n","        # u1 = deconv2d(d7, d6, self.gf*8)\n","        # u2 = deconv2d(u1, d5, self.gf*8)\n","        # u3 = deconv2d(u2, d4, self.gf*8)\n","        # u4 = deconv2d(u3, d3, self.gf*4)\n","        # u5 = deconv2d(u4, d2, self.gf*2)\n","        # u6 = deconv2d(u5, d1, self.gf)\n","\n","        # u7 = UpSampling2D(size=2)(u6)\n","        # output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n","\n","        # return Model(d0, output_img)\n","\n","        \"\"\"ResNet Generator\"\"\"\n","        def resnet_block(n_filters, input_layer):\n","            # weight initialization\n","            init = RandomNormal(stddev=0.02)\n","\n","            # first layer convolutional layer\n","            g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n","            g = InstanceNormalization(axis=-1)(g)\n","            g = Activation('relu')(g)\n","\n","            # second convolutional layer\n","            g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n","            g = InstanceNormalization(axis=-1)(g)\n","\n","            # concatenate merge channel-wise with input layer\n","            g = Concatenate()([g, input_layer])\n","\n","            return g\n","\n","        # weight initialization\n","        init = RandomNormal(stddev=0.02)\n","\n","        in_image = Input(shape=self.img_shape)\n","\n","        # c7s1-64\n","        g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n","        g = InstanceNormalization(axis=-1)(g)\n","        g = Activation('relu')(g)\n","\n","        # d128\n","        g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n","        g = InstanceNormalization(axis=-1)(g)\n","        g = Activation('relu')(g)\n","\n","        # d256\n","        g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n","        g = InstanceNormalization(axis=-1)(g)\n","        g = Activation('relu')(g)\n","\n","        # R256\n","        for _ in range(n_resnet):\n","          g = resnet_block(256, g)\n","\n","        # u128\n","        g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n","        g = InstanceNormalization(axis=-1)(g)\n","        g = Activation('relu')(g)\n","\n","        # u64\n","        g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n","        g = InstanceNormalization(axis=-1)(g)\n","        g = Activation('relu')(g)\n","\n","        # c7s1-3\n","        g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n","        g = InstanceNormalization(axis=-1)(g)\n","        out_image = Activation('tanh')(g)\n","\n","        # define model\n","        model = Model(in_image, out_image)\n","\n","        return model\n","\n","    def build_discriminator(self):\n","\n","        def d_layer(layer_input, filters, f_size=4, normalization=True):\n","            \"\"\"Discriminator layer\"\"\"\n","            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n","            if normalization:\n","                d = InstanceNormalization()(d)\n","            d = LeakyReLU(alpha=0.2)(d)\n","            return d\n","\n","        img = Input(shape=self.img_shape)\n","\n","        d1 = d_layer(img, self.df, normalization=False) # C64\n","        d2 = d_layer(d1, self.df*2) # C128\n","        d3 = d_layer(d2, self.df*4) # C256\n","        d4 = d_layer(d3, self.df*8) # C512\n","\n","        #d5 = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d4)\n","        d5 = Conv2D(self.df*8, kernel_size=4, padding='same')(d4)\n","        d5 = InstanceNormalization()(d5)\n","        d5 = LeakyReLU(alpha=0.2)(d5)\n","\n","        # patch output\n","        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d5)\n","\n","        return Model(img, validity)\n","\n","    def train(self, epochs, batch_size=1, sample_interval=-1, save_interval=-1):\n","        self.train_cnt += 1\n","\n","        print(datetime.datetime.now().isoformat(), 'Start', self.train_cnt)\n","\n","        start_time = datetime.datetime.now()\n","\n","        # Adversarial loss ground truths\n","        valid = np.ones((batch_size,) + self.disc_patch)\n","        fake = np.zeros((batch_size,) + self.disc_patch)\n","\n","        step_cnt = 1\n","\n","        #for epoch in range(epochs):\n","        for epoch in range(1, epochs+1):\n","            #for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n","            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size), 1):\n","\n","                # Translate images to opposite domain\n","                fake_B = self.g_AB.predict(imgs_A)\n","                fake_A = self.g_BA.predict(imgs_B)\n","\n","                # Train the discriminators (original images = real / translated = Fake)\n","                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n","                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n","                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n","\n","                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n","                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n","                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n","\n","                # Total disciminator loss\n","                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n","\n","                # Train the generators\n","                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n","                                                      [valid, valid,\n","                                                       imgs_A, imgs_B,\n","                                                       imgs_A, imgs_B])\n","\n","                elapsed_time = datetime.datetime.now() - start_time\n","\n","                # Plot the progress\n","                # print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" % (\n","                #     epoch, epochs,\n","                #     batch_i, self.data_loader.n_batches,\n","                #     d_loss[0], 100*d_loss[1],\n","                #     g_loss[0],\n","                #     np.mean(g_loss[1:3]),\n","                #     np.mean(g_loss[3:5]),\n","                #     np.mean(g_loss[5:6]),\n","                #     elapsed_time))\n","                self.history = self.history.append({\n","                    'epoch': epoch,\n","                    'epochs': epochs,\n","                    'batch_idx': batch_i,\n","                    'batch_num': self.data_loader.n_batches,\n","                    'd_loss': d_loss[0],\n","                    'acc': d_loss[1],\n","                    'g_loss': g_loss[0],\n","                    'adv': np.mean(g_loss[1:3]),\n","                    'recon': np.mean(g_loss[3:5]),\n","                    'id': np.mean(g_loss[5:6]),\n","                    'elapsed_time': elapsed_time\n","                }, ignore_index=True)\n","\n","                # If at save interval => save generated image samples\n","                #if sample_interval > 0 and batch_i % sample_interval == 0:\n","                if sample_interval > 0 and step_cnt % sample_interval == 0:\n","                    print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" % (\n","                        epoch, epochs,\n","                        batch_i, self.data_loader.n_batches,\n","                        d_loss[0], 100*d_loss[1],\n","                        g_loss[0],\n","                        np.mean(g_loss[1:3]),\n","                        np.mean(g_loss[3:5]),\n","                        np.mean(g_loss[5:6]),\n","                        elapsed_time))\n","\n","                    self.sample_images(epoch, batch_i)\n","\n","                #if save_interval > 0 and batch_i != 1 and (batch_i % save_interval) == 0:\n","                if save_interval > 0 and step_cnt % save_interval == 0:\n","                    file_suffix = '{}_{}_{}'.format(self.train_cnt, epoch, batch_i)\n","                    self.save_model_weights(self.combined, self.combined_name, file_suffix)\n","\n","                step_cnt += 1\n","\n","        print(datetime.datetime.now().isoformat(), 'End')\n","\n","    def generate_image_A(self, img):\n","        return self.g_AB.predict(img)\n","\n","    def generate_image_B(self, img):\n","        return self.g_BA.predict(img)\n","\n","    def sample_images(self, epoch, batch_i):\n","        dir_path = os.path.join(self.img_save_dir, self.dataset_name)\n","\n","        #os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n","        os.makedirs(dir_path, exist_ok=True)\n","\n","        r, c = 2, 3\n","\n","        imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n","        imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n","\n","        # Demo (for GIF)\n","        #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n","        #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n","\n","        # Translate images to the other domain\n","        fake_B = self.g_AB.predict(imgs_A)\n","        fake_A = self.g_BA.predict(imgs_B)\n","\n","        # Translate back to original domain\n","        reconstr_A = self.g_BA.predict(fake_B)\n","        reconstr_B = self.g_AB.predict(fake_A)\n","\n","        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n","\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","        titles = ['Original', 'Translated', 'Reconstructed']\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i, j].imshow(gen_imgs[cnt])\n","                axs[i, j].set_title(titles[j])\n","                axs[i, j].axis('off')\n","                cnt += 1\n","\n","        #fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n","        file_path = os.path.join(dir_path, '{}_{}_{}.png'.format(self.train_cnt, epoch, batch_i))\n","        fig.savefig(file_path)\n","\n","        plt.close()\n","\n","    def plot_hisotry(self, columns=[]):\n","        if len(columns) == 0:\n","            columns = ['d_loss', 'g_loss']\n","            #columns = ['d_loss', 'acc', 'g_loss', 'adv', 'recon', 'id',]\n","        self.history[columns].plot()\n","\n","    def save_models(self, file_suffix=None):\n","        self.save_model_weights(self.combined, self.combined_name, file_suffix)\n","        #self.save_model_weights(self.g_AB, self.g_AB_name, file_suffix)\n","        #self.save_model_weights(self.g_BA, self.g_BA_name, file_suffix)\n","\n","    def save_model_weights(self, model, model_name, file_suffix=None):\n","        file_path = os.path.join(self.model_save_dir, self._create_h5_file_name(model_name, file_suffix))\n","        model.save_weights(file_path)\n","\n","        print('Model weights saved.', model_name)\n","\n","    def load_models(self, file_suffix=None):\n","        self.load_model_weights(self.combined_name, file_suffix)\n","        self.load_model_weights(self.g_AB_name, file_suffix)\n","        self.load_model_weights(self.g_BA_name, file_suffix)\n","\n","    def load_model_weights(self, model_name, file_suffix=None):\n","        model = None\n","\n","        if model_name == self.combined_name:\n","            model = self.combined\n","        elif model_name == self.g_AB_name:\n","            model = self.g_AB\n","        elif model_name == self.g_BA_name:\n","            model = self.g_BA\n","        else:\n","            print('Unsupported.', model_name)\n","            return\n","\n","        if not model:\n","            print('Not initialized.', model_name)\n","            return\n","\n","        file_path = os.path.join(self.model_save_dir, self._create_h5_file_name(model_name, file_suffix))\n","\n","        if not os.path.exists(file_path):\n","            print('File Not found.', model_name)\n","            return\n","\n","        model.load_weights(file_path)\n","\n","        print('Model weights loaded.', model_name)\n","\n","    def _create_h5_file_name(self, model_name, suffix=None):\n","        if suffix:\n","            return '{}_{}.h5'.format(model_name, suffix)\n","        else:\n","            return '{}.h5'.format(model_name)"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5lusbTYOs_gW","colab":{},"executionInfo":{"status":"ok","timestamp":1599195743147,"user_tz":-540,"elapsed":6621,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["gan = CycleGAN()\n","gan.init(data_loader=data_loader)\n","\n","# Image A Count: 400, BatchSize:8, 400/8=50\n","#gan.train(epochs=1, batch_size=8, sample_interval=50, save_interval=50*5)\n","\n","#gan.history.to_csv(os.path.join(OUTPUT_DIR_PATH, 'history.csv'))"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iXhGHg5TfSmz","colab":{"base_uri":"https://localhost:8080/","height":583},"executionInfo":{"status":"ok","timestamp":1599190149030,"user_tz":-540,"elapsed":688,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}},"outputId":"c353e337-c961-47c6-b153-b0479bc3851c"},"source":["gan.d_A.summary()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Model: \"functional_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n","_________________________________________________________________\n","conv2d (Conv2D)              (None, 128, 128, 64)      3136      \n","_________________________________________________________________\n","leaky_re_lu (LeakyReLU)      (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 64, 64, 128)       131200    \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","instance_normalization (Inst (None, 64, 64, 128)       256       \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 32, 32, 256)       524544    \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 256)       0         \n","_________________________________________________________________\n","instance_normalization_1 (In (None, 32, 32, 256)       512       \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 16, 16, 512)       2097664   \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 512)       0         \n","_________________________________________________________________\n","instance_normalization_2 (In (None, 16, 16, 512)       1024      \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 16, 16, 1)         8193      \n","=================================================================\n","Total params: 2,766,529\n","Trainable params: 0\n","Non-trainable params: 2,766,529\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xh4bWpG1KOsx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":683},"executionInfo":{"status":"ok","timestamp":1599190676227,"user_tz":-540,"elapsed":872,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}},"outputId":"6267d903-3971-4266-9b19-630d4cff4b3d"},"source":["gan.d_A.summary()"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Model: \"functional_31\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_19 (InputLayer)        [(None, 256, 256, 3)]     0         \n","_________________________________________________________________\n","conv2d_118 (Conv2D)          (None, 128, 128, 64)      3136      \n","_________________________________________________________________\n","leaky_re_lu_70 (LeakyReLU)   (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","conv2d_119 (Conv2D)          (None, 64, 64, 128)       131200    \n","_________________________________________________________________\n","instance_normalization_100 ( (None, 64, 64, 128)       256       \n","_________________________________________________________________\n","leaky_re_lu_71 (LeakyReLU)   (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","conv2d_120 (Conv2D)          (None, 32, 32, 256)       524544    \n","_________________________________________________________________\n","instance_normalization_101 ( (None, 32, 32, 256)       512       \n","_________________________________________________________________\n","leaky_re_lu_72 (LeakyReLU)   (None, 32, 32, 256)       0         \n","_________________________________________________________________\n","conv2d_121 (Conv2D)          (None, 16, 16, 512)       2097664   \n","_________________________________________________________________\n","instance_normalization_102 ( (None, 16, 16, 512)       1024      \n","_________________________________________________________________\n","leaky_re_lu_73 (LeakyReLU)   (None, 16, 16, 512)       0         \n","_________________________________________________________________\n","conv2d_122 (Conv2D)          (None, 8, 8, 512)         4194816   \n","_________________________________________________________________\n","instance_normalization_103 ( (None, 8, 8, 512)         1024      \n","_________________________________________________________________\n","leaky_re_lu_74 (LeakyReLU)   (None, 8, 8, 512)         0         \n","_________________________________________________________________\n","conv2d_123 (Conv2D)          (None, 8, 8, 1)           8193      \n","=================================================================\n","Total params: 6,962,369\n","Trainable params: 0\n","Non-trainable params: 6,962,369\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5R3olmkGLERR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599192564960,"user_tz":-540,"elapsed":713,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}},"outputId":"f5a2ff49-046e-4369-9a27-1d2a492071b0"},"source":["gan.g_AB.summary()"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Model: \"functional_57\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_35 (InputLayer)           [(None, 256, 256, 3) 0                                            \n","__________________________________________________________________________________________________\n","conv2d_228 (Conv2D)             (None, 256, 256, 64) 9472        input_35[0][0]                   \n","__________________________________________________________________________________________________\n","instance_normalization_188 (Ins (None, 256, 256, 64) 128         conv2d_228[0][0]                 \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 256, 256, 64) 0           instance_normalization_188[0][0] \n","__________________________________________________________________________________________________\n","conv2d_229 (Conv2D)             (None, 128, 128, 128 73856       activation_12[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_189 (Ins (None, 128, 128, 128 256         conv2d_229[0][0]                 \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 128, 128, 128 0           instance_normalization_189[0][0] \n","__________________________________________________________________________________________________\n","conv2d_230 (Conv2D)             (None, 64, 64, 256)  295168      activation_13[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_190 (Ins (None, 64, 64, 256)  512         conv2d_230[0][0]                 \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 64, 64, 256)  0           instance_normalization_190[0][0] \n","__________________________________________________________________________________________________\n","conv2d_231 (Conv2D)             (None, 64, 64, 256)  590080      activation_14[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_191 (Ins (None, 64, 64, 256)  512         conv2d_231[0][0]                 \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 64, 64, 256)  0           instance_normalization_191[0][0] \n","__________________________________________________________________________________________________\n","conv2d_232 (Conv2D)             (None, 64, 64, 256)  590080      activation_15[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_192 (Ins (None, 64, 64, 256)  512         conv2d_232[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_57 (Concatenate)    (None, 64, 64, 512)  0           instance_normalization_192[0][0] \n","                                                                 activation_14[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_233 (Conv2D)             (None, 64, 64, 256)  1179904     concatenate_57[0][0]             \n","__________________________________________________________________________________________________\n","instance_normalization_193 (Ins (None, 64, 64, 256)  512         conv2d_233[0][0]                 \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 64, 64, 256)  0           instance_normalization_193[0][0] \n","__________________________________________________________________________________________________\n","conv2d_234 (Conv2D)             (None, 64, 64, 256)  590080      activation_16[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_194 (Ins (None, 64, 64, 256)  512         conv2d_234[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_58 (Concatenate)    (None, 64, 64, 768)  0           instance_normalization_194[0][0] \n","                                                                 concatenate_57[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_235 (Conv2D)             (None, 64, 64, 256)  1769728     concatenate_58[0][0]             \n","__________________________________________________________________________________________________\n","instance_normalization_195 (Ins (None, 64, 64, 256)  512         conv2d_235[0][0]                 \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 64, 64, 256)  0           instance_normalization_195[0][0] \n","__________________________________________________________________________________________________\n","conv2d_236 (Conv2D)             (None, 64, 64, 256)  590080      activation_17[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_196 (Ins (None, 64, 64, 256)  512         conv2d_236[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_59 (Concatenate)    (None, 64, 64, 1024) 0           instance_normalization_196[0][0] \n","                                                                 concatenate_58[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_237 (Conv2D)             (None, 64, 64, 256)  2359552     concatenate_59[0][0]             \n","__________________________________________________________________________________________________\n","instance_normalization_197 (Ins (None, 64, 64, 256)  512         conv2d_237[0][0]                 \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, 64, 64, 256)  0           instance_normalization_197[0][0] \n","__________________________________________________________________________________________________\n","conv2d_238 (Conv2D)             (None, 64, 64, 256)  590080      activation_18[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_198 (Ins (None, 64, 64, 256)  512         conv2d_238[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_60 (Concatenate)    (None, 64, 64, 1280) 0           instance_normalization_198[0][0] \n","                                                                 concatenate_59[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_239 (Conv2D)             (None, 64, 64, 256)  2949376     concatenate_60[0][0]             \n","__________________________________________________________________________________________________\n","instance_normalization_199 (Ins (None, 64, 64, 256)  512         conv2d_239[0][0]                 \n","__________________________________________________________________________________________________\n","activation_19 (Activation)      (None, 64, 64, 256)  0           instance_normalization_199[0][0] \n","__________________________________________________________________________________________________\n","conv2d_240 (Conv2D)             (None, 64, 64, 256)  590080      activation_19[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_200 (Ins (None, 64, 64, 256)  512         conv2d_240[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_61 (Concatenate)    (None, 64, 64, 1536) 0           instance_normalization_200[0][0] \n","                                                                 concatenate_60[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_241 (Conv2D)             (None, 64, 64, 256)  3539200     concatenate_61[0][0]             \n","__________________________________________________________________________________________________\n","instance_normalization_201 (Ins (None, 64, 64, 256)  512         conv2d_241[0][0]                 \n","__________________________________________________________________________________________________\n","activation_20 (Activation)      (None, 64, 64, 256)  0           instance_normalization_201[0][0] \n","__________________________________________________________________________________________________\n","conv2d_242 (Conv2D)             (None, 64, 64, 256)  590080      activation_20[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_202 (Ins (None, 64, 64, 256)  512         conv2d_242[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_62 (Concatenate)    (None, 64, 64, 1792) 0           instance_normalization_202[0][0] \n","                                                                 concatenate_61[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_243 (Conv2D)             (None, 64, 64, 256)  4129024     concatenate_62[0][0]             \n","__________________________________________________________________________________________________\n","instance_normalization_203 (Ins (None, 64, 64, 256)  512         conv2d_243[0][0]                 \n","__________________________________________________________________________________________________\n","activation_21 (Activation)      (None, 64, 64, 256)  0           instance_normalization_203[0][0] \n","__________________________________________________________________________________________________\n","conv2d_244 (Conv2D)             (None, 64, 64, 256)  590080      activation_21[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_204 (Ins (None, 64, 64, 256)  512         conv2d_244[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_63 (Concatenate)    (None, 64, 64, 2048) 0           instance_normalization_204[0][0] \n","                                                                 concatenate_62[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_245 (Conv2D)             (None, 64, 64, 256)  4718848     concatenate_63[0][0]             \n","__________________________________________________________________________________________________\n","instance_normalization_205 (Ins (None, 64, 64, 256)  512         conv2d_245[0][0]                 \n","__________________________________________________________________________________________________\n","activation_22 (Activation)      (None, 64, 64, 256)  0           instance_normalization_205[0][0] \n","__________________________________________________________________________________________________\n","conv2d_246 (Conv2D)             (None, 64, 64, 256)  590080      activation_22[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_206 (Ins (None, 64, 64, 256)  512         conv2d_246[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_64 (Concatenate)    (None, 64, 64, 2304) 0           instance_normalization_206[0][0] \n","                                                                 concatenate_63[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_247 (Conv2D)             (None, 64, 64, 256)  5308672     concatenate_64[0][0]             \n","__________________________________________________________________________________________________\n","instance_normalization_207 (Ins (None, 64, 64, 256)  512         conv2d_247[0][0]                 \n","__________________________________________________________________________________________________\n","activation_23 (Activation)      (None, 64, 64, 256)  0           instance_normalization_207[0][0] \n","__________________________________________________________________________________________________\n","conv2d_248 (Conv2D)             (None, 64, 64, 256)  590080      activation_23[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_208 (Ins (None, 64, 64, 256)  512         conv2d_248[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_65 (Concatenate)    (None, 64, 64, 2560) 0           instance_normalization_208[0][0] \n","                                                                 concatenate_64[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_transpose (Conv2DTranspo (None, 128, 128, 128 2949248     concatenate_65[0][0]             \n","__________________________________________________________________________________________________\n","instance_normalization_209 (Ins (None, 128, 128, 128 256         conv2d_transpose[0][0]           \n","__________________________________________________________________________________________________\n","activation_24 (Activation)      (None, 128, 128, 128 0           instance_normalization_209[0][0] \n","__________________________________________________________________________________________________\n","conv2d_transpose_1 (Conv2DTrans (None, 256, 256, 64) 73792       activation_24[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_210 (Ins (None, 256, 256, 64) 128         conv2d_transpose_1[0][0]         \n","__________________________________________________________________________________________________\n","activation_25 (Activation)      (None, 256, 256, 64) 0           instance_normalization_210[0][0] \n","__________________________________________________________________________________________________\n","conv2d_249 (Conv2D)             (None, 256, 256, 3)  9411        activation_25[0][0]              \n","__________________________________________________________________________________________________\n","instance_normalization_211 (Ins (None, 256, 256, 3)  6           conv2d_249[0][0]                 \n","__________________________________________________________________________________________________\n","activation_26 (Activation)      (None, 256, 256, 3)  0           instance_normalization_211[0][0] \n","==================================================================================================\n","Total params: 35,276,553\n","Trainable params: 35,276,553\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mezykEPCTgHE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1599193605232,"user_tz":-540,"elapsed":3340,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}},"outputId":"896f176a-a7c5-4524-8f22-7e049e61d881"},"source":["# Image A Count: 400, BatchSize:1, 400/1=400\n","gan.train(epochs=1, batch_size=1, sample_interval=1, save_interval=-1)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["2020-09-04T04:26:42.171254 Start 2\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-8ce6de79fdd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Image A Count: 400, BatchSize:1, 400/1=400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-31-5b3ebd9d58e6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval, save_interval)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;31m# Train the discriminators (original images = real / translated = Fake)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mdA_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m                 \u001b[0mdA_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0mdA_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdA_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                                     class_weight)\n\u001b[1;32m   1694\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 3210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3142\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1195 mean_squared_error\n        return K.mean(math_ops.squared_difference(y_pred, y_true), axis=-1)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10399 squared_difference\n        \"SquaredDifference\", x=x, y=y, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n        compute_device)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\n        op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1975 __init__\n        control_input_ops, op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 8 and 16 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](functional_63/conv2d_277/BiasAdd, IteratorGetNext:1)' with input shapes: [1,8,8,1], [1,16,16,1].\n"]}]},{"cell_type":"code","metadata":{"id":"pF2emfwnWLjh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":683},"executionInfo":{"status":"ok","timestamp":1599195751699,"user_tz":-540,"elapsed":739,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}},"outputId":"fcaa1633-f42e-43be-bb86-359fd835c64a"},"source":["gan.d_A.summary()"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Model: \"functional_73\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_46 (InputLayer)        [(None, 256, 256, 3)]     0         \n","_________________________________________________________________\n","conv2d_332 (Conv2D)          (None, 128, 128, 64)      3136      \n","_________________________________________________________________\n","leaky_re_lu_148 (LeakyReLU)  (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","conv2d_333 (Conv2D)          (None, 64, 64, 128)       131200    \n","_________________________________________________________________\n","instance_normalization_295 ( (None, 64, 64, 128)       256       \n","_________________________________________________________________\n","leaky_re_lu_149 (LeakyReLU)  (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","conv2d_334 (Conv2D)          (None, 32, 32, 256)       524544    \n","_________________________________________________________________\n","instance_normalization_296 ( (None, 32, 32, 256)       512       \n","_________________________________________________________________\n","leaky_re_lu_150 (LeakyReLU)  (None, 32, 32, 256)       0         \n","_________________________________________________________________\n","conv2d_335 (Conv2D)          (None, 16, 16, 512)       2097664   \n","_________________________________________________________________\n","instance_normalization_297 ( (None, 16, 16, 512)       1024      \n","_________________________________________________________________\n","leaky_re_lu_151 (LeakyReLU)  (None, 16, 16, 512)       0         \n","_________________________________________________________________\n","conv2d_336 (Conv2D)          (None, 16, 16, 512)       4194816   \n","_________________________________________________________________\n","instance_normalization_298 ( (None, 16, 16, 512)       1024      \n","_________________________________________________________________\n","leaky_re_lu_152 (LeakyReLU)  (None, 16, 16, 512)       0         \n","_________________________________________________________________\n","conv2d_337 (Conv2D)          (None, 16, 16, 1)         8193      \n","=================================================================\n","Total params: 6,962,369\n","Trainable params: 0\n","Non-trainable params: 6,962,369\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bW67IHp5Yj8i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599197044449,"user_tz":-540,"elapsed":1271416,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}},"outputId":"bc641825-7a7d-41f9-e1a2-581fd6cd1432"},"source":["# Image A Count: 400, BatchSize:1, 400/1=400\n","gan.train(epochs=1, batch_size=1, sample_interval=1, save_interval=-1)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["2020-09-04T05:02:53.254110 Start 1\n","[Epoch 1/1] [Batch 1/400] [D loss: 1.942661, acc:  34%] [G loss: 33.633678, adv: 9.408720, recon: 0.675806, id: 0.626350] time: 0:00:37.098750 \n","[Epoch 1/1] [Batch 2/400] [D loss: 2.795672, acc:  48%] [G loss: 72.562340, adv: 28.539177, recon: 0.703559, id: 0.621616] time: 0:00:46.428541 \n","[Epoch 1/1] [Batch 3/400] [D loss: 9.764742, acc:  51%] [G loss: 65.247063, adv: 24.976495, recon: 0.695483, id: 0.688303] time: 0:00:50.709669 \n","[Epoch 1/1] [Batch 4/400] [D loss: 9.667971, acc:  50%] [G loss: 92.740280, adv: 38.790462, recon: 0.690390, id: 0.625036] time: 0:00:54.224089 \n","[Epoch 1/1] [Batch 5/400] [D loss: 15.036783, acc:  56%] [G loss: 78.719299, adv: 31.828210, recon: 0.686425, id: 0.623199] time: 0:00:58.181195 \n","[Epoch 1/1] [Batch 6/400] [D loss: 15.358995, acc:  48%] [G loss: 45.784721, adv: 14.962354, recon: 0.720172, id: 0.724992] time: 0:01:01.460293 \n","[Epoch 1/1] [Batch 7/400] [D loss: 10.889147, acc:  51%] [G loss: 19.875343, adv: 2.959367, recon: 0.633906, id: 0.619918] time: 0:01:04.943755 \n","[Epoch 1/1] [Batch 8/400] [D loss: 4.405699, acc:  46%] [G loss: 17.357506, adv: 1.437337, recon: 0.661999, id: 0.574808] time: 0:01:09.293421 \n","[Epoch 1/1] [Batch 9/400] [D loss: 2.243376, acc:  47%] [G loss: 14.893344, adv: 0.777603, recon: 0.602712, id: 0.588388] time: 0:01:12.430265 \n","[Epoch 1/1] [Batch 10/400] [D loss: 1.083729, acc:  46%] [G loss: 14.235901, adv: 0.603029, recon: 0.587362, id: 0.568995] time: 0:01:16.843126 \n","[Epoch 1/1] [Batch 11/400] [D loss: 1.036594, acc:  48%] [G loss: 14.221307, adv: 0.517621, recon: 0.593065, id: 0.580201] time: 0:01:20.964599 \n","[Epoch 1/1] [Batch 12/400] [D loss: 0.630926, acc:  53%] [G loss: 12.846202, adv: 0.487632, recon: 0.533576, id: 0.511919] time: 0:01:24.493949 \n","[Epoch 1/1] [Batch 13/400] [D loss: 0.532348, acc:  49%] [G loss: 12.822567, adv: 0.510470, recon: 0.529866, id: 0.570899] time: 0:01:28.058469 \n","[Epoch 1/1] [Batch 14/400] [D loss: 0.597364, acc:  46%] [G loss: 13.961554, adv: 0.481969, recon: 0.586710, id: 0.510587] time: 0:01:32.046839 \n","[Epoch 1/1] [Batch 15/400] [D loss: 0.515811, acc:  46%] [G loss: 10.767301, adv: 0.414617, recon: 0.443688, id: 0.485742] time: 0:01:35.895167 \n","[Epoch 1/1] [Batch 16/400] [D loss: 0.501939, acc:  49%] [G loss: 12.131939, adv: 0.485465, recon: 0.498641, id: 0.492281] time: 0:01:39.830986 \n","[Epoch 1/1] [Batch 17/400] [D loss: 0.460006, acc:  48%] [G loss: 10.579283, adv: 0.422941, recon: 0.436123, id: 0.423475] time: 0:01:43.269275 \n","[Epoch 1/1] [Batch 18/400] [D loss: 0.412647, acc:  52%] [G loss: 11.741784, adv: 0.458111, recon: 0.484680, id: 0.460132] time: 0:01:47.130820 \n","[Epoch 1/1] [Batch 19/400] [D loss: 0.419462, acc:  49%] [G loss: 12.225035, adv: 0.474964, recon: 0.506780, id: 0.408795] time: 0:01:49.823180 \n","[Epoch 1/1] [Batch 20/400] [D loss: 0.470206, acc:  46%] [G loss: 10.202760, adv: 0.411955, recon: 0.420497, id: 0.462088] time: 0:01:53.567288 \n","[Epoch 1/1] [Batch 21/400] [D loss: 0.437910, acc:  47%] [G loss: 10.980967, adv: 0.406300, recon: 0.457725, id: 0.452285] time: 0:01:57.988923 \n","[Epoch 1/1] [Batch 22/400] [D loss: 0.432039, acc:  52%] [G loss: 11.217354, adv: 0.479799, recon: 0.459520, id: 0.464522] time: 0:02:01.284628 \n","[Epoch 1/1] [Batch 23/400] [D loss: 0.408420, acc:  48%] [G loss: 9.762335, adv: 0.467374, recon: 0.392920, id: 0.493105] time: 0:02:05.181127 \n","[Epoch 1/1] [Batch 24/400] [D loss: 0.387286, acc:  45%] [G loss: 9.730050, adv: 0.405909, recon: 0.397710, id: 0.432450] time: 0:02:09.384902 \n","[Epoch 1/1] [Batch 25/400] [D loss: 0.432933, acc:  44%] [G loss: 10.279173, adv: 0.423120, recon: 0.422158, id: 0.440433] time: 0:02:12.734895 \n","[Epoch 1/1] [Batch 26/400] [D loss: 0.446983, acc:  47%] [G loss: 10.583306, adv: 0.430186, recon: 0.433864, id: 0.428721] time: 0:02:16.510150 \n","[Epoch 1/1] [Batch 27/400] [D loss: 0.399597, acc:  50%] [G loss: 10.565695, adv: 0.487032, recon: 0.426220, id: 0.491679] time: 0:02:20.431028 \n","[Epoch 1/1] [Batch 28/400] [D loss: 0.348209, acc:  49%] [G loss: 9.482676, adv: 0.495068, recon: 0.378129, id: 0.400532] time: 0:02:23.656063 \n","[Epoch 1/1] [Batch 29/400] [D loss: 0.455225, acc:  45%] [G loss: 9.857864, adv: 0.432550, recon: 0.402586, id: 0.507710] time: 0:02:26.773290 \n","[Epoch 1/1] [Batch 30/400] [D loss: 0.395342, acc:  50%] [G loss: 12.280663, adv: 0.464086, recon: 0.504687, id: 0.570788] time: 0:02:30.519845 \n","[Epoch 1/1] [Batch 31/400] [D loss: 0.397925, acc:  46%] [G loss: 11.069137, adv: 0.461043, recon: 0.454612, id: 0.385544] time: 0:02:33.971771 \n","[Epoch 1/1] [Batch 32/400] [D loss: 0.337589, acc:  52%] [G loss: 9.938246, adv: 0.506723, recon: 0.401059, id: 0.382497] time: 0:02:37.214540 \n","[Epoch 1/1] [Batch 33/400] [D loss: 0.372725, acc:  50%] [G loss: 10.553223, adv: 0.481054, recon: 0.428163, id: 0.467631] time: 0:02:40.059179 \n","[Epoch 1/1] [Batch 34/400] [D loss: 0.358730, acc:  50%] [G loss: 8.874956, adv: 0.415764, recon: 0.363364, id: 0.301789] time: 0:02:43.467131 \n","[Epoch 1/1] [Batch 35/400] [D loss: 0.348433, acc:  52%] [G loss: 10.443762, adv: 0.468874, recon: 0.427520, id: 0.352323] time: 0:02:47.017886 \n","[Epoch 1/1] [Batch 36/400] [D loss: 0.425092, acc:  50%] [G loss: 12.056191, adv: 0.466636, recon: 0.503055, id: 0.432591] time: 0:02:50.195769 \n","[Epoch 1/1] [Batch 37/400] [D loss: 0.396048, acc:  49%] [G loss: 10.966812, adv: 0.488942, recon: 0.450512, id: 0.417596] time: 0:02:53.619403 \n","[Epoch 1/1] [Batch 38/400] [D loss: 0.393281, acc:  48%] [G loss: 9.576010, adv: 0.492031, recon: 0.387727, id: 0.411989] time: 0:02:57.204369 \n","[Epoch 1/1] [Batch 39/400] [D loss: 0.351438, acc:  48%] [G loss: 10.940708, adv: 0.484586, recon: 0.450618, id: 0.510456] time: 0:03:01.759245 \n","[Epoch 1/1] [Batch 40/400] [D loss: 0.447762, acc:  43%] [G loss: 10.485008, adv: 0.430637, recon: 0.437059, id: 0.320325] time: 0:03:05.531021 \n","[Epoch 1/1] [Batch 41/400] [D loss: 0.366597, acc:  53%] [G loss: 10.957697, adv: 0.513938, recon: 0.449344, id: 0.419468] time: 0:03:09.541075 \n","[Epoch 1/1] [Batch 42/400] [D loss: 0.354841, acc:  51%] [G loss: 9.842214, adv: 0.505929, recon: 0.397113, id: 0.394125] time: 0:03:13.783894 \n","[Epoch 1/1] [Batch 43/400] [D loss: 0.418928, acc:  46%] [G loss: 9.034391, adv: 0.433819, recon: 0.369541, id: 0.334409] time: 0:03:17.199652 \n","[Epoch 1/1] [Batch 44/400] [D loss: 0.445219, acc:  45%] [G loss: 11.336406, adv: 0.435510, recon: 0.473850, id: 0.466195] time: 0:03:20.509213 \n","[Epoch 1/1] [Batch 45/400] [D loss: 0.365449, acc:  51%] [G loss: 9.834711, adv: 0.511629, recon: 0.396466, id: 0.457240] time: 0:03:24.221101 \n","[Epoch 1/1] [Batch 46/400] [D loss: 0.348378, acc:  47%] [G loss: 9.527518, adv: 0.459474, recon: 0.390289, id: 0.373498] time: 0:03:28.146833 \n","[Epoch 1/1] [Batch 47/400] [D loss: 0.390082, acc:  44%] [G loss: 11.477207, adv: 0.497036, recon: 0.475467, id: 0.443999] time: 0:03:31.207880 \n","[Epoch 1/1] [Batch 48/400] [D loss: 0.358491, acc:  51%] [G loss: 10.789637, adv: 0.494165, recon: 0.443565, id: 0.414240] time: 0:03:34.477111 \n","[Epoch 1/1] [Batch 49/400] [D loss: 0.386969, acc:  44%] [G loss: 9.114580, adv: 0.452503, recon: 0.369606, id: 0.414784] time: 0:03:39.623906 \n","[Epoch 1/1] [Batch 50/400] [D loss: 0.465098, acc:  37%] [G loss: 9.682833, adv: 0.474279, recon: 0.395850, id: 0.460282] time: 0:03:42.635646 \n","[Epoch 1/1] [Batch 51/400] [D loss: 0.371548, acc:  53%] [G loss: 9.754639, adv: 0.537905, recon: 0.392614, id: 0.398716] time: 0:03:45.998694 \n","[Epoch 1/1] [Batch 52/400] [D loss: 0.439916, acc:  42%] [G loss: 8.579098, adv: 0.413917, recon: 0.350039, id: 0.311210] time: 0:03:48.857945 \n","[Epoch 1/1] [Batch 53/400] [D loss: 0.368655, acc:  49%] [G loss: 10.436897, adv: 0.486399, recon: 0.430676, id: 0.263867] time: 0:03:53.138019 \n","[Epoch 1/1] [Batch 54/400] [D loss: 0.358323, acc:  45%] [G loss: 9.433296, adv: 0.465821, recon: 0.383588, id: 0.310076] time: 0:03:56.987437 \n","[Epoch 1/1] [Batch 55/400] [D loss: 0.412388, acc:  44%] [G loss: 11.252624, adv: 0.449683, recon: 0.469496, id: 0.460812] time: 0:04:00.534423 \n","[Epoch 1/1] [Batch 56/400] [D loss: 0.351027, acc:  48%] [G loss: 10.042181, adv: 0.509201, recon: 0.406757, id: 0.365886] time: 0:04:03.917583 \n","[Epoch 1/1] [Batch 57/400] [D loss: 0.423020, acc:  44%] [G loss: 8.782495, adv: 0.422131, recon: 0.360596, id: 0.340887] time: 0:04:08.346259 \n","[Epoch 1/1] [Batch 58/400] [D loss: 0.368372, acc:  45%] [G loss: 9.070030, adv: 0.449897, recon: 0.369560, id: 0.410365] time: 0:04:12.200747 \n","[Epoch 1/1] [Batch 59/400] [D loss: 0.336708, acc:  50%] [G loss: 9.820541, adv: 0.492804, recon: 0.398402, id: 0.430058] time: 0:04:15.969535 \n","[Epoch 1/1] [Batch 60/400] [D loss: 0.377831, acc:  41%] [G loss: 10.258817, adv: 0.429552, recon: 0.425453, id: 0.398061] time: 0:04:19.169767 \n","[Epoch 1/1] [Batch 61/400] [D loss: 0.344579, acc:  48%] [G loss: 10.646767, adv: 0.481831, recon: 0.438333, id: 0.386374] time: 0:04:22.875311 \n","[Epoch 1/1] [Batch 62/400] [D loss: 0.352202, acc:  43%] [G loss: 8.583366, adv: 0.427665, recon: 0.349908, id: 0.343089] time: 0:04:25.722910 \n","[Epoch 1/1] [Batch 63/400] [D loss: 0.345622, acc:  47%] [G loss: 8.789865, adv: 0.456850, recon: 0.356840, id: 0.337418] time: 0:04:28.432037 \n","[Epoch 1/1] [Batch 64/400] [D loss: 0.405710, acc:  41%] [G loss: 9.366249, adv: 0.434825, recon: 0.383823, id: 0.374949] time: 0:04:31.845458 \n","[Epoch 1/1] [Batch 65/400] [D loss: 0.471217, acc:  36%] [G loss: 8.275103, adv: 0.444295, recon: 0.334764, id: 0.369081] time: 0:04:35.893852 \n","[Epoch 1/1] [Batch 66/400] [D loss: 0.348643, acc:  44%] [G loss: 9.290365, adv: 0.506172, recon: 0.374964, id: 0.395751] time: 0:04:40.328245 \n","[Epoch 1/1] [Batch 67/400] [D loss: 0.386124, acc:  42%] [G loss: 9.571110, adv: 0.422429, recon: 0.394184, id: 0.403116] time: 0:04:43.759610 \n","[Epoch 1/1] [Batch 68/400] [D loss: 0.431271, acc:  39%] [G loss: 6.862432, adv: 0.428180, recon: 0.272536, id: 0.242153] time: 0:04:47.478753 \n","[Epoch 1/1] [Batch 69/400] [D loss: 0.365648, acc:  50%] [G loss: 7.749034, adv: 0.503736, recon: 0.305364, id: 0.277365] time: 0:04:51.175792 \n","[Epoch 1/1] [Batch 70/400] [D loss: 0.340606, acc:  46%] [G loss: 9.557108, adv: 0.474671, recon: 0.388201, id: 0.392294] time: 0:04:53.992441 \n","[Epoch 1/1] [Batch 71/400] [D loss: 0.452509, acc:  32%] [G loss: 9.342527, adv: 0.384235, recon: 0.389117, id: 0.283610] time: 0:04:56.754750 \n","[Epoch 1/1] [Batch 72/400] [D loss: 0.346994, acc:  47%] [G loss: 10.439622, adv: 0.464371, recon: 0.434231, id: 0.340211] time: 0:05:00.600876 \n","[Epoch 1/1] [Batch 73/400] [D loss: 0.382655, acc:  43%] [G loss: 8.911465, adv: 0.465786, recon: 0.357012, id: 0.377183] time: 0:05:02.702501 \n","[Epoch 1/1] [Batch 74/400] [D loss: 0.389063, acc:  47%] [G loss: 10.371033, adv: 0.498016, recon: 0.426717, id: 0.365687] time: 0:05:05.654779 \n","[Epoch 1/1] [Batch 75/400] [D loss: 0.378841, acc:  43%] [G loss: 9.871493, adv: 0.506256, recon: 0.402202, id: 0.416640] time: 0:05:09.456204 \n","[Epoch 1/1] [Batch 76/400] [D loss: 0.385195, acc:  45%] [G loss: 8.829164, adv: 0.470614, recon: 0.357814, id: 0.360933] time: 0:05:13.035142 \n","[Epoch 1/1] [Batch 77/400] [D loss: 0.378468, acc:  45%] [G loss: 8.645266, adv: 0.462752, recon: 0.349122, id: 0.377144] time: 0:05:16.311868 \n","[Epoch 1/1] [Batch 78/400] [D loss: 0.410683, acc:  40%] [G loss: 9.235021, adv: 0.421880, recon: 0.378760, id: 0.331069] time: 0:05:20.470322 \n","[Epoch 1/1] [Batch 79/400] [D loss: 0.355046, acc:  47%] [G loss: 8.658875, adv: 0.455612, recon: 0.350594, id: 0.276112] time: 0:05:23.191897 \n","[Epoch 1/1] [Batch 80/400] [D loss: 0.378772, acc:  38%] [G loss: 7.345068, adv: 0.444134, recon: 0.293289, id: 0.275007] time: 0:05:26.647160 \n","[Epoch 1/1] [Batch 81/400] [D loss: 0.371217, acc:  39%] [G loss: 8.283035, adv: 0.421815, recon: 0.337979, id: 0.322671] time: 0:05:29.785229 \n","[Epoch 1/1] [Batch 82/400] [D loss: 0.368684, acc:  38%] [G loss: 8.206635, adv: 0.459876, recon: 0.330097, id: 0.240937] time: 0:05:33.104962 \n","[Epoch 1/1] [Batch 83/400] [D loss: 0.342464, acc:  44%] [G loss: 7.656989, adv: 0.479546, recon: 0.302309, id: 0.328658] time: 0:05:36.440453 \n","[Epoch 1/1] [Batch 84/400] [D loss: 0.330421, acc:  43%] [G loss: 9.181385, adv: 0.464415, recon: 0.375903, id: 0.233120] time: 0:05:39.843422 \n","[Epoch 1/1] [Batch 85/400] [D loss: 0.372604, acc:  44%] [G loss: 8.727264, adv: 0.459569, recon: 0.353775, id: 0.310831] time: 0:05:44.455994 \n","[Epoch 1/1] [Batch 86/400] [D loss: 0.359934, acc:  43%] [G loss: 9.084688, adv: 0.464792, recon: 0.366531, id: 0.394891] time: 0:05:48.598934 \n","[Epoch 1/1] [Batch 87/400] [D loss: 0.347160, acc:  44%] [G loss: 7.929039, adv: 0.465402, recon: 0.318064, id: 0.234973] time: 0:05:52.569795 \n","[Epoch 1/1] [Batch 88/400] [D loss: 0.363379, acc:  42%] [G loss: 8.671826, adv: 0.449898, recon: 0.352244, id: 0.364313] time: 0:05:56.144963 \n","[Epoch 1/1] [Batch 89/400] [D loss: 0.352927, acc:  41%] [G loss: 7.142090, adv: 0.450632, recon: 0.280291, id: 0.308411] time: 0:05:58.879705 \n","[Epoch 1/1] [Batch 90/400] [D loss: 0.408003, acc:  35%] [G loss: 7.909038, adv: 0.446753, recon: 0.316373, id: 0.347255] time: 0:06:03.203779 \n","[Epoch 1/1] [Batch 91/400] [D loss: 0.407508, acc:  35%] [G loss: 8.412289, adv: 0.481290, recon: 0.337543, id: 0.366347] time: 0:06:06.748378 \n","[Epoch 1/1] [Batch 92/400] [D loss: 0.388413, acc:  38%] [G loss: 8.113112, adv: 0.475099, recon: 0.325711, id: 0.376959] time: 0:06:09.601505 \n","[Epoch 1/1] [Batch 93/400] [D loss: 0.368415, acc:  42%] [G loss: 8.204699, adv: 0.447678, recon: 0.331615, id: 0.322321] time: 0:06:12.369476 \n","[Epoch 1/1] [Batch 94/400] [D loss: 0.341960, acc:  43%] [G loss: 6.811871, adv: 0.465605, recon: 0.266487, id: 0.244693] time: 0:06:16.186429 \n","[Epoch 1/1] [Batch 95/400] [D loss: 0.396124, acc:  37%] [G loss: 8.594005, adv: 0.409386, recon: 0.352952, id: 0.341474] time: 0:06:20.504305 \n","[Epoch 1/1] [Batch 96/400] [D loss: 0.306625, acc:  52%] [G loss: 8.576162, adv: 0.534976, recon: 0.340296, id: 0.362658] time: 0:06:23.228082 \n","[Epoch 1/1] [Batch 97/400] [D loss: 0.283447, acc:  52%] [G loss: 7.099133, adv: 0.514755, recon: 0.273520, id: 0.250294] time: 0:06:26.328041 \n","[Epoch 1/1] [Batch 98/400] [D loss: 0.428022, acc:  39%] [G loss: 8.640261, adv: 0.444620, recon: 0.351962, id: 0.340509] time: 0:06:29.708931 \n","[Epoch 1/1] [Batch 99/400] [D loss: 0.387259, acc:  46%] [G loss: 9.497957, adv: 0.504883, recon: 0.386757, id: 0.303699] time: 0:06:31.951651 \n","[Epoch 1/1] [Batch 100/400] [D loss: 0.299703, acc:  53%] [G loss: 9.078082, adv: 0.563059, recon: 0.360513, id: 0.334637] time: 0:06:34.749067 \n","[Epoch 1/1] [Batch 101/400] [D loss: 0.359502, acc:  42%] [G loss: 8.237589, adv: 0.468317, recon: 0.331065, id: 0.301107] time: 0:06:37.575931 \n","[Epoch 1/1] [Batch 102/400] [D loss: 0.453023, acc:  39%] [G loss: 8.155830, adv: 0.402069, recon: 0.332031, id: 0.402798] time: 0:06:40.194394 \n","[Epoch 1/1] [Batch 103/400] [D loss: 0.336025, acc:  52%] [G loss: 8.292262, adv: 0.529586, recon: 0.327464, id: 0.272854] time: 0:06:43.329832 \n","[Epoch 1/1] [Batch 104/400] [D loss: 0.425016, acc:  39%] [G loss: 9.379116, adv: 0.422016, recon: 0.388618, id: 0.328386] time: 0:06:47.084803 \n","[Epoch 1/1] [Batch 105/400] [D loss: 0.344159, acc:  49%] [G loss: 9.568802, adv: 0.498345, recon: 0.390258, id: 0.381522] time: 0:06:50.483503 \n","[Epoch 1/1] [Batch 106/400] [D loss: 0.429617, acc:  39%] [G loss: 7.671288, adv: 0.431841, recon: 0.309257, id: 0.269319] time: 0:06:53.559795 \n","[Epoch 1/1] [Batch 107/400] [D loss: 0.374596, acc:  42%] [G loss: 8.899182, adv: 0.466225, recon: 0.359623, id: 0.407807] time: 0:06:56.262631 \n","[Epoch 1/1] [Batch 108/400] [D loss: 0.381259, acc:  43%] [G loss: 7.893397, adv: 0.434862, recon: 0.316704, id: 0.349623] time: 0:07:00.962220 \n","[Epoch 1/1] [Batch 109/400] [D loss: 0.375599, acc:  37%] [G loss: 7.364758, adv: 0.471966, recon: 0.288307, id: 0.339155] time: 0:07:04.636572 \n","[Epoch 1/1] [Batch 110/400] [D loss: 0.349112, acc:  46%] [G loss: 9.479080, adv: 0.507581, recon: 0.382908, id: 0.453629] time: 0:07:07.644112 \n","[Epoch 1/1] [Batch 111/400] [D loss: 0.361951, acc:  42%] [G loss: 9.748311, adv: 0.469997, recon: 0.402255, id: 0.376976] time: 0:07:11.598442 \n","[Epoch 1/1] [Batch 112/400] [D loss: 0.398611, acc:  34%] [G loss: 6.916372, adv: 0.436179, recon: 0.272157, id: 0.244703] time: 0:07:14.632785 \n","[Epoch 1/1] [Batch 113/400] [D loss: 0.342605, acc:  43%] [G loss: 8.831543, adv: 0.457095, recon: 0.360101, id: 0.292397] time: 0:07:16.867314 \n","[Epoch 1/1] [Batch 114/400] [D loss: 0.321591, acc:  50%] [G loss: 8.055837, adv: 0.493406, recon: 0.318809, id: 0.346746] time: 0:07:21.519163 \n","[Epoch 1/1] [Batch 115/400] [D loss: 0.406593, acc:  41%] [G loss: 8.377236, adv: 0.497164, recon: 0.333057, id: 0.396764] time: 0:07:23.741566 \n","[Epoch 1/1] [Batch 116/400] [D loss: 0.400028, acc:  38%] [G loss: 8.589644, adv: 0.406779, recon: 0.352877, id: 0.296392] time: 0:07:27.158026 \n","[Epoch 1/1] [Batch 117/400] [D loss: 0.352162, acc:  42%] [G loss: 7.640447, adv: 0.456463, recon: 0.302443, id: 0.368305] time: 0:07:30.187268 \n","[Epoch 1/1] [Batch 118/400] [D loss: 0.336405, acc:  42%] [G loss: 8.401090, adv: 0.492051, recon: 0.337653, id: 0.285082] time: 0:07:32.460730 \n","[Epoch 1/1] [Batch 119/400] [D loss: 0.408313, acc:  36%] [G loss: 7.827250, adv: 0.466690, recon: 0.313325, id: 0.328373] time: 0:07:34.845645 \n","[Epoch 1/1] [Batch 120/400] [D loss: 0.288882, acc:  52%] [G loss: 7.084352, adv: 0.482316, recon: 0.277063, id: 0.271144] time: 0:07:37.442526 \n","[Epoch 1/1] [Batch 121/400] [D loss: 0.358022, acc:  45%] [G loss: 8.296724, adv: 0.459309, recon: 0.335748, id: 0.311525] time: 0:07:40.770323 \n","[Epoch 1/1] [Batch 122/400] [D loss: 0.294436, acc:  54%] [G loss: 8.320169, adv: 0.532382, recon: 0.329485, id: 0.267440] time: 0:07:43.769022 \n","[Epoch 1/1] [Batch 123/400] [D loss: 0.317550, acc:  44%] [G loss: 7.431615, adv: 0.477879, recon: 0.291877, id: 0.297767] time: 0:07:46.521161 \n","[Epoch 1/1] [Batch 124/400] [D loss: 0.247955, acc:  59%] [G loss: 8.651705, adv: 0.553551, recon: 0.340745, id: 0.278822] time: 0:07:49.836253 \n","[Epoch 1/1] [Batch 125/400] [D loss: 0.355649, acc:  45%] [G loss: 8.525667, adv: 0.451394, recon: 0.346111, id: 0.290430] time: 0:07:52.970850 \n","[Epoch 1/1] [Batch 126/400] [D loss: 0.273516, acc:  54%] [G loss: 8.875189, adv: 0.514485, recon: 0.355604, id: 0.374201] time: 0:07:55.727885 \n","[Epoch 1/1] [Batch 127/400] [D loss: 0.346030, acc:  45%] [G loss: 7.801993, adv: 0.437569, recon: 0.314731, id: 0.279243] time: 0:07:59.199476 \n","[Epoch 1/1] [Batch 128/400] [D loss: 0.367942, acc:  36%] [G loss: 8.144426, adv: 0.452648, recon: 0.328801, id: 0.268011] time: 0:08:01.757532 \n","[Epoch 1/1] [Batch 129/400] [D loss: 0.364456, acc:  44%] [G loss: 9.366863, adv: 0.529731, recon: 0.376342, id: 0.364340] time: 0:08:04.577372 \n","[Epoch 1/1] [Batch 130/400] [D loss: 0.398775, acc:  37%] [G loss: 6.852905, adv: 0.420282, recon: 0.272165, id: 0.259271] time: 0:08:07.386194 \n","[Epoch 1/1] [Batch 131/400] [D loss: 0.448927, acc:  28%] [G loss: 7.375414, adv: 0.421305, recon: 0.295579, id: 0.273786] time: 0:08:11.446771 \n","[Epoch 1/1] [Batch 132/400] [D loss: 0.377852, acc:  37%] [G loss: 6.953938, adv: 0.510021, recon: 0.267845, id: 0.263657] time: 0:08:14.939015 \n","[Epoch 1/1] [Batch 133/400] [D loss: 0.428752, acc:  37%] [G loss: 7.629735, adv: 0.455730, recon: 0.304223, id: 0.328552] time: 0:08:18.936077 \n","[Epoch 1/1] [Batch 134/400] [D loss: 0.399389, acc:  36%] [G loss: 8.145575, adv: 0.452489, recon: 0.329010, id: 0.393979] time: 0:08:22.963321 \n","[Epoch 1/1] [Batch 135/400] [D loss: 0.348492, acc:  44%] [G loss: 8.805131, adv: 0.476105, recon: 0.356399, id: 0.349162] time: 0:08:25.173330 \n","[Epoch 1/1] [Batch 136/400] [D loss: 0.345792, acc:  45%] [G loss: 7.535781, adv: 0.457348, recon: 0.302297, id: 0.211787] time: 0:08:28.969181 \n","[Epoch 1/1] [Batch 137/400] [D loss: 0.344337, acc:  46%] [G loss: 7.446539, adv: 0.453339, recon: 0.297463, id: 0.303291] time: 0:08:31.582101 \n","[Epoch 1/1] [Batch 138/400] [D loss: 0.307384, acc:  48%] [G loss: 7.870556, adv: 0.518269, recon: 0.310242, id: 0.317857] time: 0:08:34.153060 \n","[Epoch 1/1] [Batch 139/400] [D loss: 0.395397, acc:  31%] [G loss: 8.388007, adv: 0.455197, recon: 0.339558, id: 0.347155] time: 0:08:36.912507 \n","[Epoch 1/1] [Batch 140/400] [D loss: 0.399928, acc:  44%] [G loss: 8.328337, adv: 0.476048, recon: 0.335758, id: 0.372548] time: 0:08:40.606363 \n","[Epoch 1/1] [Batch 141/400] [D loss: 0.364565, acc:  39%] [G loss: 9.046823, adv: 0.463162, recon: 0.367570, id: 0.372005] time: 0:08:43.147659 \n","[Epoch 1/1] [Batch 142/400] [D loss: 0.331617, acc:  45%] [G loss: 7.578986, adv: 0.511161, recon: 0.298702, id: 0.246304] time: 0:08:45.828677 \n","[Epoch 1/1] [Batch 143/400] [D loss: 0.354307, acc:  46%] [G loss: 8.746319, adv: 0.486386, recon: 0.353110, id: 0.286791] time: 0:08:49.696806 \n","[Epoch 1/1] [Batch 144/400] [D loss: 0.404616, acc:  37%] [G loss: 7.279831, adv: 0.404300, recon: 0.293611, id: 0.289614] time: 0:08:51.704791 \n","[Epoch 1/1] [Batch 145/400] [D loss: 0.364285, acc:  41%] [G loss: 7.763662, adv: 0.510228, recon: 0.304964, id: 0.336072] time: 0:08:55.075901 \n","[Epoch 1/1] [Batch 146/400] [D loss: 0.399010, acc:  38%] [G loss: 10.552147, adv: 0.494485, recon: 0.435368, id: 0.476148] time: 0:08:58.670713 \n","[Epoch 1/1] [Batch 147/400] [D loss: 0.306416, acc:  57%] [G loss: 10.276274, adv: 0.558596, recon: 0.417926, id: 0.504286] time: 0:09:01.309548 \n","[Epoch 1/1] [Batch 148/400] [D loss: 0.421115, acc:  36%] [G loss: 10.226089, adv: 0.472506, recon: 0.422926, id: 0.356112] time: 0:09:04.783121 \n","[Epoch 1/1] [Batch 149/400] [D loss: 0.384860, acc:  43%] [G loss: 9.453860, adv: 0.446563, recon: 0.388702, id: 0.374236] time: 0:09:07.445867 \n","[Epoch 1/1] [Batch 150/400] [D loss: 0.359247, acc:  42%] [G loss: 7.498002, adv: 0.494377, recon: 0.296163, id: 0.293747] time: 0:09:10.131760 \n","[Epoch 1/1] [Batch 151/400] [D loss: 0.335944, acc:  45%] [G loss: 7.831459, adv: 0.486595, recon: 0.312404, id: 0.288383] time: 0:09:12.960870 \n","[Epoch 1/1] [Batch 152/400] [D loss: 0.417697, acc:  33%] [G loss: 8.886580, adv: 0.409248, recon: 0.366404, id: 0.347163] time: 0:09:16.529393 \n","[Epoch 1/1] [Batch 153/400] [D loss: 0.287529, acc:  53%] [G loss: 7.352656, adv: 0.558421, recon: 0.284498, id: 0.215257] time: 0:09:19.883365 \n","[Epoch 1/1] [Batch 154/400] [D loss: 0.410767, acc:  30%] [G loss: 6.413764, adv: 0.440212, recon: 0.251987, id: 0.245890] time: 0:09:22.582721 \n","[Epoch 1/1] [Batch 155/400] [D loss: 0.297219, acc:  48%] [G loss: 7.958494, adv: 0.514086, recon: 0.315509, id: 0.296037] time: 0:09:25.338526 \n","[Epoch 1/1] [Batch 156/400] [D loss: 0.304461, acc:  48%] [G loss: 7.500072, adv: 0.477218, recon: 0.297703, id: 0.233194] time: 0:09:28.311907 \n","[Epoch 1/1] [Batch 157/400] [D loss: 0.302824, acc:  46%] [G loss: 7.548209, adv: 0.452390, recon: 0.302722, id: 0.285827] time: 0:09:30.531467 \n","[Epoch 1/1] [Batch 158/400] [D loss: 0.374900, acc:  37%] [G loss: 6.885483, adv: 0.465564, recon: 0.271342, id: 0.264184] time: 0:09:33.408422 \n","[Epoch 1/1] [Batch 159/400] [D loss: 0.346492, acc:  41%] [G loss: 7.937789, adv: 0.487503, recon: 0.316997, id: 0.265680] time: 0:09:36.717306 \n","[Epoch 1/1] [Batch 160/400] [D loss: 0.443069, acc:  33%] [G loss: 7.682904, adv: 0.435593, recon: 0.309072, id: 0.248490] time: 0:09:39.555446 \n","[Epoch 1/1] [Batch 161/400] [D loss: 0.365337, acc:  43%] [G loss: 7.723323, adv: 0.459022, recon: 0.309310, id: 0.244453] time: 0:09:42.524117 \n","[Epoch 1/1] [Batch 162/400] [D loss: 0.364010, acc:  45%] [G loss: 7.163160, adv: 0.446845, recon: 0.285452, id: 0.216979] time: 0:09:44.702979 \n","[Epoch 1/1] [Batch 163/400] [D loss: 0.340865, acc:  47%] [G loss: 7.231297, adv: 0.491498, recon: 0.283841, id: 0.217193] time: 0:09:47.454421 \n","[Epoch 1/1] [Batch 164/400] [D loss: 0.322757, acc:  49%] [G loss: 8.803499, adv: 0.496202, recon: 0.355311, id: 0.331986] time: 0:09:50.398112 \n","[Epoch 1/1] [Batch 165/400] [D loss: 0.406904, acc:  34%] [G loss: 7.642292, adv: 0.421575, recon: 0.309559, id: 0.248522] time: 0:09:53.671538 \n","[Epoch 1/1] [Batch 166/400] [D loss: 0.310840, acc:  52%] [G loss: 8.070627, adv: 0.567433, recon: 0.315394, id: 0.247614] time: 0:09:57.229272 \n","[Epoch 1/1] [Batch 167/400] [D loss: 0.371885, acc:  40%] [G loss: 9.226185, adv: 0.463553, recon: 0.378201, id: 0.466657] time: 0:09:59.803010 \n","[Epoch 1/1] [Batch 168/400] [D loss: 0.381361, acc:  42%] [G loss: 7.723733, adv: 0.445703, recon: 0.310877, id: 0.319513] time: 0:10:04.055580 \n","[Epoch 1/1] [Batch 169/400] [D loss: 0.368601, acc:  38%] [G loss: 9.665487, adv: 0.495546, recon: 0.393291, id: 0.486374] time: 0:10:07.695953 \n","[Epoch 1/1] [Batch 170/400] [D loss: 0.329726, acc:  46%] [G loss: 8.880047, adv: 0.510848, recon: 0.356797, id: 0.358751] time: 0:10:10.829093 \n","[Epoch 1/1] [Batch 171/400] [D loss: 0.361491, acc:  38%] [G loss: 6.952719, adv: 0.447501, recon: 0.275517, id: 0.231909] time: 0:10:13.726039 \n","[Epoch 1/1] [Batch 172/400] [D loss: 0.292933, acc:  51%] [G loss: 7.786705, adv: 0.526383, recon: 0.306202, id: 0.246211] time: 0:10:17.148080 \n","[Epoch 1/1] [Batch 173/400] [D loss: 0.325747, acc:  46%] [G loss: 7.876474, adv: 0.559726, recon: 0.305855, id: 0.327825] time: 0:10:19.329532 \n","[Epoch 1/1] [Batch 174/400] [D loss: 0.353508, acc:  46%] [G loss: 8.735313, adv: 0.514718, recon: 0.349747, id: 0.309635] time: 0:10:22.924022 \n","[Epoch 1/1] [Batch 175/400] [D loss: 0.365644, acc:  50%] [G loss: 8.694884, adv: 0.508318, recon: 0.350175, id: 0.300595] time: 0:10:26.354543 \n","[Epoch 1/1] [Batch 176/400] [D loss: 0.437957, acc:  36%] [G loss: 10.684704, adv: 0.493006, recon: 0.441402, id: 0.597998] time: 0:10:29.403395 \n","[Epoch 1/1] [Batch 177/400] [D loss: 0.416653, acc:  34%] [G loss: 7.579391, adv: 0.457805, recon: 0.301804, id: 0.359906] time: 0:10:32.370509 \n","[Epoch 1/1] [Batch 178/400] [D loss: 0.359458, acc:  45%] [G loss: 9.994928, adv: 0.471175, recon: 0.411638, id: 0.434580] time: 0:10:34.633766 \n","[Epoch 1/1] [Batch 179/400] [D loss: 0.403106, acc:  36%] [G loss: 8.274616, adv: 0.413127, recon: 0.339752, id: 0.327232] time: 0:10:37.655366 \n","[Epoch 1/1] [Batch 180/400] [D loss: 0.352256, acc:  39%] [G loss: 6.996556, adv: 0.507138, recon: 0.271417, id: 0.283202] time: 0:10:41.631105 \n","[Epoch 1/1] [Batch 181/400] [D loss: 0.298792, acc:  50%] [G loss: 6.615546, adv: 0.534439, recon: 0.251808, id: 0.261545] time: 0:10:44.251270 \n","[Epoch 1/1] [Batch 182/400] [D loss: 0.395733, acc:  33%] [G loss: 8.907448, adv: 0.449884, recon: 0.364016, id: 0.386558] time: 0:10:47.283086 \n","[Epoch 1/1] [Batch 183/400] [D loss: 0.363058, acc:  40%] [G loss: 6.672853, adv: 0.474663, recon: 0.259752, id: 0.216995] time: 0:10:50.647004 \n","[Epoch 1/1] [Batch 184/400] [D loss: 0.324437, acc:  44%] [G loss: 7.631098, adv: 0.478856, recon: 0.303181, id: 0.291329] time: 0:10:53.315819 \n","[Epoch 1/1] [Batch 185/400] [D loss: 0.349375, acc:  45%] [G loss: 7.655282, adv: 0.511616, recon: 0.301740, id: 0.244814] time: 0:10:56.688668 \n","[Epoch 1/1] [Batch 186/400] [D loss: 0.337048, acc:  44%] [G loss: 8.072992, adv: 0.490047, recon: 0.321923, id: 0.328433] time: 0:10:59.644984 \n","[Epoch 1/1] [Batch 187/400] [D loss: 0.363362, acc:  40%] [G loss: 8.343927, adv: 0.479764, recon: 0.336204, id: 0.339252] time: 0:11:01.743989 \n","[Epoch 1/1] [Batch 188/400] [D loss: 0.356635, acc:  40%] [G loss: 7.621925, adv: 0.480914, recon: 0.302932, id: 0.246829] time: 0:11:05.167836 \n","[Epoch 1/1] [Batch 189/400] [D loss: 0.413552, acc:  34%] [G loss: 7.776134, adv: 0.419185, recon: 0.315587, id: 0.286595] time: 0:11:07.813017 \n","[Epoch 1/1] [Batch 190/400] [D loss: 0.379613, acc:  38%] [G loss: 5.551043, adv: 0.464105, recon: 0.209720, id: 0.178829] time: 0:11:11.594611 \n","[Epoch 1/1] [Batch 191/400] [D loss: 0.323457, acc:  44%] [G loss: 7.323166, adv: 0.476494, recon: 0.290660, id: 0.302694] time: 0:11:15.361374 \n","[Epoch 1/1] [Batch 192/400] [D loss: 0.421699, acc:  38%] [G loss: 10.229361, adv: 0.504428, recon: 0.419301, id: 0.364986] time: 0:11:18.863932 \n","[Epoch 1/1] [Batch 193/400] [D loss: 0.367864, acc:  50%] [G loss: 6.407500, adv: 0.521772, recon: 0.244454, id: 0.231044] time: 0:11:21.413107 \n","[Epoch 1/1] [Batch 194/400] [D loss: 0.408063, acc:  33%] [G loss: 7.848966, adv: 0.517687, recon: 0.310560, id: 0.256795] time: 0:11:23.594759 \n","[Epoch 1/1] [Batch 195/400] [D loss: 0.431304, acc:  45%] [G loss: 8.311336, adv: 0.446518, recon: 0.337361, id: 0.430251] time: 0:11:26.758295 \n","[Epoch 1/1] [Batch 196/400] [D loss: 0.289376, acc:  49%] [G loss: 8.557240, adv: 0.536408, recon: 0.341246, id: 0.305415] time: 0:11:30.214256 \n","[Epoch 1/1] [Batch 197/400] [D loss: 0.354296, acc:  40%] [G loss: 6.897637, adv: 0.479513, recon: 0.270096, id: 0.227410] time: 0:11:33.842403 \n","[Epoch 1/1] [Batch 198/400] [D loss: 0.385508, acc:  35%] [G loss: 7.692817, adv: 0.441031, recon: 0.309324, id: 0.310803] time: 0:11:36.451631 \n","[Epoch 1/1] [Batch 199/400] [D loss: 0.325605, acc:  47%] [G loss: 7.359881, adv: 0.555119, recon: 0.284269, id: 0.208277] time: 0:11:39.395097 \n","[Epoch 1/1] [Batch 200/400] [D loss: 0.363286, acc:  43%] [G loss: 7.559738, adv: 0.493427, recon: 0.298455, id: 0.312149] time: 0:11:42.070100 \n","[Epoch 1/1] [Batch 201/400] [D loss: 0.408514, acc:  40%] [G loss: 6.705899, adv: 0.491931, recon: 0.259314, id: 0.245647] time: 0:11:45.200763 \n","[Epoch 1/1] [Batch 202/400] [D loss: 0.312777, acc:  46%] [G loss: 6.733804, adv: 0.536090, recon: 0.256763, id: 0.229084] time: 0:11:47.991604 \n","[Epoch 1/1] [Batch 203/400] [D loss: 0.369235, acc:  37%] [G loss: 8.247208, adv: 0.496977, recon: 0.328803, id: 0.319529] time: 0:11:50.675902 \n","[Epoch 1/1] [Batch 204/400] [D loss: 0.396667, acc:  47%] [G loss: 8.516600, adv: 0.547420, recon: 0.335776, id: 0.447868] time: 0:11:53.644778 \n","[Epoch 1/1] [Batch 205/400] [D loss: 0.344514, acc:  46%] [G loss: 7.890695, adv: 0.498112, recon: 0.313663, id: 0.349140] time: 0:11:55.827358 \n","[Epoch 1/1] [Batch 206/400] [D loss: 0.347808, acc:  50%] [G loss: 9.428223, adv: 0.531119, recon: 0.380568, id: 0.299606] time: 0:11:59.795209 \n","[Epoch 1/1] [Batch 207/400] [D loss: 0.385071, acc:  41%] [G loss: 8.303041, adv: 0.498430, recon: 0.331239, id: 0.324343] time: 0:12:03.185864 \n","[Epoch 1/1] [Batch 208/400] [D loss: 0.383938, acc:  35%] [G loss: 7.289001, adv: 0.450604, recon: 0.289601, id: 0.239995] time: 0:12:06.331221 \n","[Epoch 1/1] [Batch 209/400] [D loss: 0.368017, acc:  42%] [G loss: 8.706450, adv: 0.471542, recon: 0.352484, id: 0.373964] time: 0:12:08.539604 \n","[Epoch 1/1] [Batch 210/400] [D loss: 0.374862, acc:  37%] [G loss: 6.225190, adv: 0.483054, recon: 0.237245, id: 0.245316] time: 0:12:11.808833 \n","[Epoch 1/1] [Batch 211/400] [D loss: 0.354721, acc:  41%] [G loss: 8.013223, adv: 0.486108, recon: 0.320947, id: 0.225197] time: 0:12:15.091250 \n","[Epoch 1/1] [Batch 212/400] [D loss: 0.360753, acc:  40%] [G loss: 6.513386, adv: 0.521310, recon: 0.246336, id: 0.266542] time: 0:12:18.016884 \n","[Epoch 1/1] [Batch 213/400] [D loss: 0.259701, acc:  60%] [G loss: 10.153833, adv: 0.579926, recon: 0.408414, id: 0.473451] time: 0:12:20.667261 \n","[Epoch 1/1] [Batch 214/400] [D loss: 0.391088, acc:  44%] [G loss: 8.741715, adv: 0.491683, recon: 0.352993, id: 0.313113] time: 0:12:23.373674 \n","[Epoch 1/1] [Batch 215/400] [D loss: 0.411128, acc:  38%] [G loss: 6.575347, adv: 0.473083, recon: 0.255268, id: 0.241031] time: 0:12:27.048913 \n","[Epoch 1/1] [Batch 216/400] [D loss: 0.359031, acc:  47%] [G loss: 9.153869, adv: 0.487533, recon: 0.371276, id: 0.373717] time: 0:12:29.289173 \n","[Epoch 1/1] [Batch 217/400] [D loss: 0.392078, acc:  44%] [G loss: 9.473860, adv: 0.520383, recon: 0.384208, id: 0.341275] time: 0:12:31.006010 \n","[Epoch 1/1] [Batch 218/400] [D loss: 0.252294, acc:  64%] [G loss: 8.303709, adv: 0.704122, recon: 0.312626, id: 0.382109] time: 0:12:35.449790 \n","[Epoch 1/1] [Batch 219/400] [D loss: 0.427331, acc:  44%] [G loss: 6.315928, adv: 0.506412, recon: 0.240214, id: 0.246821] time: 0:12:38.267870 \n","[Epoch 1/1] [Batch 220/400] [D loss: 0.287381, acc:  62%] [G loss: 7.946938, adv: 0.586822, recon: 0.307901, id: 0.315504] time: 0:12:41.155264 \n","[Epoch 1/1] [Batch 221/400] [D loss: 0.325433, acc:  51%] [G loss: 8.943348, adv: 0.526389, recon: 0.361405, id: 0.332692] time: 0:12:43.587377 \n","[Epoch 1/1] [Batch 222/400] [D loss: 0.289891, acc:  61%] [G loss: 11.322235, adv: 0.749025, recon: 0.451443, id: 0.294213] time: 0:12:45.821604 \n","[Epoch 1/1] [Batch 223/400] [D loss: 0.307014, acc:  59%] [G loss: 8.994848, adv: 0.586507, recon: 0.356905, id: 0.332058] time: 0:12:49.304917 \n","[Epoch 1/1] [Batch 224/400] [D loss: 0.326808, acc:  53%] [G loss: 7.521722, adv: 0.535495, recon: 0.294396, id: 0.204388] time: 0:12:51.238998 \n","[Epoch 1/1] [Batch 225/400] [D loss: 0.393388, acc:  37%] [G loss: 8.533927, adv: 0.442856, recon: 0.348537, id: 0.303467] time: 0:12:53.427306 \n","[Epoch 1/1] [Batch 226/400] [D loss: 0.230403, acc:  67%] [G loss: 8.162151, adv: 0.631354, recon: 0.313647, id: 0.271932] time: 0:12:56.765836 \n","[Epoch 1/1] [Batch 227/400] [D loss: 0.362300, acc:  41%] [G loss: 6.441956, adv: 0.456628, recon: 0.251211, id: 0.205593] time: 0:13:00.892167 \n","[Epoch 1/1] [Batch 228/400] [D loss: 0.380902, acc:  40%] [G loss: 7.528694, adv: 0.509091, recon: 0.295980, id: 0.289549] time: 0:13:03.975785 \n","[Epoch 1/1] [Batch 229/400] [D loss: 0.320200, acc:  50%] [G loss: 8.090350, adv: 0.514415, recon: 0.319989, id: 0.321218] time: 0:13:08.457714 \n","[Epoch 1/1] [Batch 230/400] [D loss: 0.315586, acc:  54%] [G loss: 7.405983, adv: 0.552426, recon: 0.285110, id: 0.292580] time: 0:13:12.310232 \n","[Epoch 1/1] [Batch 231/400] [D loss: 0.399567, acc:  43%] [G loss: 9.948767, adv: 0.509235, recon: 0.406676, id: 0.372571] time: 0:13:14.400396 \n","[Epoch 1/1] [Batch 232/400] [D loss: 0.381263, acc:  43%] [G loss: 9.153994, adv: 0.530600, recon: 0.367883, id: 0.346250] time: 0:13:17.564627 \n","[Epoch 1/1] [Batch 233/400] [D loss: 0.378400, acc:  41%] [G loss: 8.052872, adv: 0.485033, recon: 0.322770, id: 0.301872] time: 0:13:20.391542 \n","[Epoch 1/1] [Batch 234/400] [D loss: 0.360325, acc:  42%] [G loss: 7.184589, adv: 0.486898, recon: 0.280019, id: 0.293108] time: 0:13:24.056151 \n","[Epoch 1/1] [Batch 235/400] [D loss: 0.305307, acc:  50%] [G loss: 8.226789, adv: 0.586237, recon: 0.320889, id: 0.309660] time: 0:13:27.530051 \n","[Epoch 1/1] [Batch 236/400] [D loss: 0.320165, acc:  50%] [G loss: 8.295906, adv: 0.501961, recon: 0.330812, id: 0.355208] time: 0:13:29.912064 \n","[Epoch 1/1] [Batch 237/400] [D loss: 0.305617, acc:  50%] [G loss: 8.600388, adv: 0.511226, recon: 0.344834, id: 0.420788] time: 0:13:33.746714 \n","[Epoch 1/1] [Batch 238/400] [D loss: 0.280391, acc:  53%] [G loss: 7.342243, adv: 0.581650, recon: 0.279581, id: 0.256501] time: 0:13:37.776859 \n","[Epoch 1/1] [Batch 239/400] [D loss: 0.304756, acc:  50%] [G loss: 6.981696, adv: 0.520714, recon: 0.267490, id: 0.286527] time: 0:13:39.297225 \n","[Epoch 1/1] [Batch 240/400] [D loss: 0.240440, acc:  66%] [G loss: 8.524387, adv: 0.636240, recon: 0.328838, id: 0.291608] time: 0:13:41.980694 \n","[Epoch 1/1] [Batch 241/400] [D loss: 0.286275, acc:  56%] [G loss: 8.341192, adv: 0.544357, recon: 0.328500, id: 0.291006] time: 0:13:45.631572 \n","[Epoch 1/1] [Batch 242/400] [D loss: 0.352496, acc:  50%] [G loss: 8.258427, adv: 0.547953, recon: 0.324555, id: 0.325053] time: 0:13:49.379545 \n","[Epoch 1/1] [Batch 243/400] [D loss: 0.352767, acc:  41%] [G loss: 7.960515, adv: 0.527924, recon: 0.312955, id: 0.357883] time: 0:13:52.176280 \n","[Epoch 1/1] [Batch 244/400] [D loss: 0.274563, acc:  56%] [G loss: 8.201777, adv: 0.671504, recon: 0.312114, id: 0.210436] time: 0:13:54.416063 \n","[Epoch 1/1] [Batch 245/400] [D loss: 0.400230, acc:  35%] [G loss: 9.286291, adv: 0.439102, recon: 0.381447, id: 0.484533] time: 0:13:57.342029 \n","[Epoch 1/1] [Batch 246/400] [D loss: 0.275562, acc:  58%] [G loss: 9.398973, adv: 0.575589, recon: 0.377030, id: 0.388328] time: 0:14:00.903461 \n","[Epoch 1/1] [Batch 247/400] [D loss: 0.417378, acc:  35%] [G loss: 7.414896, adv: 0.468339, recon: 0.294914, id: 0.283318] time: 0:14:03.924746 \n","[Epoch 1/1] [Batch 248/400] [D loss: 0.310981, acc:  56%] [G loss: 7.072603, adv: 0.604059, recon: 0.266758, id: 0.231583] time: 0:14:06.137255 \n","[Epoch 1/1] [Batch 249/400] [D loss: 0.346265, acc:  47%] [G loss: 7.485848, adv: 0.522560, recon: 0.292024, id: 0.330110] time: 0:14:08.831055 \n","[Epoch 1/1] [Batch 250/400] [D loss: 0.464683, acc:  33%] [G loss: 6.160358, adv: 0.467307, recon: 0.235727, id: 0.234518] time: 0:14:11.459263 \n","[Epoch 1/1] [Batch 251/400] [D loss: 0.347578, acc:  43%] [G loss: 7.670185, adv: 0.502452, recon: 0.302343, id: 0.289220] time: 0:14:14.044992 \n","[Epoch 1/1] [Batch 252/400] [D loss: 0.327431, acc:  53%] [G loss: 8.693342, adv: 0.568468, recon: 0.345759, id: 0.257769] time: 0:14:18.201075 \n","[Epoch 1/1] [Batch 253/400] [D loss: 0.362812, acc:  50%] [G loss: 7.846019, adv: 0.521995, recon: 0.308950, id: 0.257287] time: 0:14:21.433605 \n","[Epoch 1/1] [Batch 254/400] [D loss: 0.456562, acc:  34%] [G loss: 8.164369, adv: 0.492280, recon: 0.327331, id: 0.264050] time: 0:14:23.613967 \n","[Epoch 1/1] [Batch 255/400] [D loss: 0.376838, acc:  42%] [G loss: 7.669818, adv: 0.539548, recon: 0.298854, id: 0.284133] time: 0:14:25.124131 \n","[Epoch 1/1] [Batch 256/400] [D loss: 0.271237, acc:  59%] [G loss: 8.610146, adv: 0.623125, recon: 0.332498, id: 0.344449] time: 0:14:27.562737 \n","[Epoch 1/1] [Batch 257/400] [D loss: 0.370472, acc:  46%] [G loss: 7.394701, adv: 0.483700, recon: 0.290725, id: 0.277757] time: 0:14:29.837654 \n","[Epoch 1/1] [Batch 258/400] [D loss: 0.420067, acc:  34%] [G loss: 6.635693, adv: 0.443661, recon: 0.260282, id: 0.206060] time: 0:14:32.105306 \n","[Epoch 1/1] [Batch 259/400] [D loss: 0.227522, acc:  65%] [G loss: 7.744184, adv: 0.626741, recon: 0.294968, id: 0.256007] time: 0:14:34.943676 \n","[Epoch 1/1] [Batch 260/400] [D loss: 0.267531, acc:  57%] [G loss: 8.013035, adv: 0.553822, recon: 0.313819, id: 0.262121] time: 0:14:38.443447 \n","[Epoch 1/1] [Batch 261/400] [D loss: 0.376356, acc:  45%] [G loss: 8.057013, adv: 0.491130, recon: 0.322074, id: 0.320007] time: 0:14:40.821686 \n","[Epoch 1/1] [Batch 262/400] [D loss: 0.305376, acc:  50%] [G loss: 7.785762, adv: 0.501743, recon: 0.307568, id: 0.299954] time: 0:14:43.683782 \n","[Epoch 1/1] [Batch 263/400] [D loss: 0.402243, acc:  45%] [G loss: 6.683642, adv: 0.479234, recon: 0.257240, id: 0.316513] time: 0:14:46.932852 \n","[Epoch 1/1] [Batch 264/400] [D loss: 0.397695, acc:  42%] [G loss: 7.957163, adv: 0.506278, recon: 0.314329, id: 0.398566] time: 0:14:50.198446 \n","[Epoch 1/1] [Batch 265/400] [D loss: 0.238481, acc:  62%] [G loss: 8.813822, adv: 0.599248, recon: 0.345001, id: 0.440727] time: 0:14:52.714178 \n","[Epoch 1/1] [Batch 266/400] [D loss: 0.266824, acc:  63%] [G loss: 9.543378, adv: 0.560952, recon: 0.381949, id: 0.322375] time: 0:14:54.921466 \n","[Epoch 1/1] [Batch 267/400] [D loss: 0.349853, acc:  44%] [G loss: 7.579127, adv: 0.528472, recon: 0.297540, id: 0.296449] time: 0:14:57.496461 \n","[Epoch 1/1] [Batch 268/400] [D loss: 0.343405, acc:  50%] [G loss: 9.971368, adv: 0.487826, recon: 0.408955, id: 0.399850] time: 0:15:01.275305 \n","[Epoch 1/1] [Batch 269/400] [D loss: 0.336729, acc:  47%] [G loss: 7.638906, adv: 0.532326, recon: 0.298390, id: 0.270025] time: 0:15:03.985309 \n","[Epoch 1/1] [Batch 270/400] [D loss: 0.319271, acc:  54%] [G loss: 7.571511, adv: 0.553140, recon: 0.294104, id: 0.296926] time: 0:15:07.678901 \n","[Epoch 1/1] [Batch 271/400] [D loss: 0.363830, acc:  41%] [G loss: 7.343376, adv: 0.516464, recon: 0.286360, id: 0.312411] time: 0:15:11.034640 \n","[Epoch 1/1] [Batch 272/400] [D loss: 0.309757, acc:  50%] [G loss: 7.311523, adv: 0.559153, recon: 0.282162, id: 0.218377] time: 0:15:13.809153 \n","[Epoch 1/1] [Batch 273/400] [D loss: 0.272263, acc:  58%] [G loss: 6.434265, adv: 0.594805, recon: 0.238604, id: 0.209098] time: 0:15:16.416175 \n","[Epoch 1/1] [Batch 274/400] [D loss: 0.337999, acc:  43%] [G loss: 7.166171, adv: 0.546363, recon: 0.278164, id: 0.233373] time: 0:15:19.726198 \n","[Epoch 1/1] [Batch 275/400] [D loss: 0.421340, acc:  35%] [G loss: 6.613655, adv: 0.436587, recon: 0.262129, id: 0.245461] time: 0:15:22.374241 \n","[Epoch 1/1] [Batch 276/400] [D loss: 0.260807, acc:  54%] [G loss: 7.267024, adv: 0.630902, recon: 0.272978, id: 0.298928] time: 0:15:25.024286 \n","[Epoch 1/1] [Batch 277/400] [D loss: 0.363872, acc:  48%] [G loss: 8.552631, adv: 0.515914, recon: 0.342721, id: 0.314496] time: 0:15:27.731187 \n","[Epoch 1/1] [Batch 278/400] [D loss: 0.382356, acc:  40%] [G loss: 6.905113, adv: 0.477891, recon: 0.270182, id: 0.252236] time: 0:15:29.916489 \n","[Epoch 1/1] [Batch 279/400] [D loss: 0.277637, acc:  54%] [G loss: 6.705890, adv: 0.610502, recon: 0.249067, id: 0.206423] time: 0:15:32.249612 \n","[Epoch 1/1] [Batch 280/400] [D loss: 0.264080, acc:  55%] [G loss: 7.756907, adv: 0.568975, recon: 0.301474, id: 0.284082] time: 0:15:35.960879 \n","[Epoch 1/1] [Batch 281/400] [D loss: 0.220169, acc:  65%] [G loss: 7.084598, adv: 0.563626, recon: 0.272068, id: 0.217644] time: 0:15:39.570443 \n","[Epoch 1/1] [Batch 282/400] [D loss: 0.305480, acc:  47%] [G loss: 7.429761, adv: 0.517395, recon: 0.293271, id: 0.222704] time: 0:15:42.097185 \n","[Epoch 1/1] [Batch 283/400] [D loss: 0.443991, acc:  36%] [G loss: 7.664359, adv: 0.469537, recon: 0.305829, id: 0.336090] time: 0:15:44.176316 \n","[Epoch 1/1] [Batch 284/400] [D loss: 0.408777, acc:  34%] [G loss: 6.826776, adv: 0.453639, recon: 0.268305, id: 0.251878] time: 0:15:46.834324 \n","[Epoch 1/1] [Batch 285/400] [D loss: 0.380705, acc:  36%] [G loss: 7.047739, adv: 0.487955, recon: 0.275393, id: 0.283281] time: 0:15:49.552941 \n","[Epoch 1/1] [Batch 286/400] [D loss: 0.365168, acc:  37%] [G loss: 6.135020, adv: 0.490310, recon: 0.232266, id: 0.276121] time: 0:15:53.060979 \n","[Epoch 1/1] [Batch 287/400] [D loss: 0.231248, acc:  66%] [G loss: 10.163939, adv: 0.662206, recon: 0.402991, id: 0.338734] time: 0:15:55.798712 \n","[Epoch 1/1] [Batch 288/400] [D loss: 0.452323, acc:  39%] [G loss: 8.582771, adv: 0.487065, recon: 0.345367, id: 0.369702] time: 0:15:59.512105 \n","[Epoch 1/1] [Batch 289/400] [D loss: 0.250151, acc:  59%] [G loss: 7.053679, adv: 0.616578, recon: 0.262942, id: 0.269227] time: 0:16:02.020390 \n","[Epoch 1/1] [Batch 290/400] [D loss: 0.319083, acc:  49%] [G loss: 7.332816, adv: 0.531595, recon: 0.285481, id: 0.239616] time: 0:16:04.810994 \n","[Epoch 1/1] [Batch 291/400] [D loss: 0.278735, acc:  55%] [G loss: 7.323316, adv: 0.623889, recon: 0.274603, id: 0.319766] time: 0:16:07.666465 \n","[Epoch 1/1] [Batch 292/400] [D loss: 0.327605, acc:  44%] [G loss: 6.498119, adv: 0.536436, recon: 0.245854, id: 0.193559] time: 0:16:10.947568 \n","[Epoch 1/1] [Batch 293/400] [D loss: 0.330609, acc:  50%] [G loss: 8.722174, adv: 0.557929, recon: 0.345251, id: 0.332823] time: 0:16:13.529432 \n","[Epoch 1/1] [Batch 294/400] [D loss: 0.436559, acc:  38%] [G loss: 7.728916, adv: 0.500886, recon: 0.304317, id: 0.313614] time: 0:16:15.472326 \n","[Epoch 1/1] [Batch 295/400] [D loss: 0.455163, acc:  30%] [G loss: 5.447174, adv: 0.451096, recon: 0.203186, id: 0.202009] time: 0:16:18.221870 \n","[Epoch 1/1] [Batch 296/400] [D loss: 0.441294, acc:  31%] [G loss: 7.315289, adv: 0.444347, recon: 0.291072, id: 0.317493] time: 0:16:20.668792 \n","[Epoch 1/1] [Batch 297/400] [D loss: 0.287144, acc:  57%] [G loss: 7.022218, adv: 0.577802, recon: 0.263466, id: 0.291652] time: 0:16:23.314838 \n","[Epoch 1/1] [Batch 298/400] [D loss: 0.262015, acc:  59%] [G loss: 7.807083, adv: 0.548813, recon: 0.303781, id: 0.324362] time: 0:16:26.108230 \n","[Epoch 1/1] [Batch 299/400] [D loss: 0.543339, acc:  20%] [G loss: 6.011899, adv: 0.378913, recon: 0.236554, id: 0.235934] time: 0:16:29.393539 \n","[Epoch 1/1] [Batch 300/400] [D loss: 0.269238, acc:  54%] [G loss: 7.719948, adv: 0.576140, recon: 0.296500, id: 0.261596] time: 0:16:32.666126 \n","[Epoch 1/1] [Batch 301/400] [D loss: 0.296261, acc:  50%] [G loss: 6.784131, adv: 0.566799, recon: 0.256872, id: 0.243535] time: 0:16:35.551212 \n","[Epoch 1/1] [Batch 302/400] [D loss: 0.418887, acc:  35%] [G loss: 6.451595, adv: 0.496662, recon: 0.248155, id: 0.177624] time: 0:16:37.552334 \n","[Epoch 1/1] [Batch 303/400] [D loss: 0.298665, acc:  51%] [G loss: 9.112812, adv: 0.534703, recon: 0.367292, id: 0.312069] time: 0:16:40.278012 \n","[Epoch 1/1] [Batch 304/400] [D loss: 0.293021, acc:  48%] [G loss: 8.372855, adv: 0.554164, recon: 0.329888, id: 0.171876] time: 0:16:42.853521 \n","[Epoch 1/1] [Batch 305/400] [D loss: 0.220414, acc:  68%] [G loss: 7.427014, adv: 0.609324, recon: 0.283733, id: 0.180877] time: 0:16:47.053057 \n","[Epoch 1/1] [Batch 306/400] [D loss: 0.326438, acc:  43%] [G loss: 7.161140, adv: 0.494576, recon: 0.281274, id: 0.226312] time: 0:16:50.193777 \n","[Epoch 1/1] [Batch 307/400] [D loss: 0.221122, acc:  67%] [G loss: 10.939857, adv: 0.714612, recon: 0.432476, id: 0.475297] time: 0:16:53.047786 \n","[Epoch 1/1] [Batch 308/400] [D loss: 0.429761, acc:  44%] [G loss: 7.534117, adv: 0.536964, recon: 0.294415, id: 0.340523] time: 0:16:55.166737 \n","[Epoch 1/1] [Batch 309/400] [D loss: 0.299875, acc:  55%] [G loss: 7.339152, adv: 0.588244, recon: 0.283644, id: 0.292902] time: 0:16:58.143538 \n","[Epoch 1/1] [Batch 310/400] [D loss: 0.410541, acc:  38%] [G loss: 7.256940, adv: 0.445351, recon: 0.290728, id: 0.219635] time: 0:17:01.127781 \n","[Epoch 1/1] [Batch 311/400] [D loss: 0.140076, acc:  83%] [G loss: 9.273756, adv: 0.738809, recon: 0.357052, id: 0.274945] time: 0:17:03.860692 \n","[Epoch 1/1] [Batch 312/400] [D loss: 0.267666, acc:  57%] [G loss: 7.587968, adv: 0.556004, recon: 0.295680, id: 0.254097] time: 0:17:06.863210 \n","[Epoch 1/1] [Batch 313/400] [D loss: 0.239830, acc:  61%] [G loss: 7.711706, adv: 0.549722, recon: 0.301608, id: 0.303503] time: 0:17:10.174927 \n","[Epoch 1/1] [Batch 314/400] [D loss: 0.249263, acc:  59%] [G loss: 7.395855, adv: 0.538554, recon: 0.288014, id: 0.227164] time: 0:17:12.912867 \n","[Epoch 1/1] [Batch 315/400] [D loss: 0.230715, acc:  66%] [G loss: 7.279001, adv: 0.637509, recon: 0.273612, id: 0.286710] time: 0:17:15.204820 \n","[Epoch 1/1] [Batch 316/400] [D loss: 0.220765, acc:  67%] [G loss: 8.226773, adv: 0.606032, recon: 0.320059, id: 0.265072] time: 0:17:17.904638 \n","[Epoch 1/1] [Batch 317/400] [D loss: 0.260656, acc:  62%] [G loss: 8.602749, adv: 0.547990, recon: 0.341579, id: 0.373703] time: 0:17:20.774506 \n","[Epoch 1/1] [Batch 318/400] [D loss: 0.265900, acc:  57%] [G loss: 6.974220, adv: 0.511918, recon: 0.270983, id: 0.228142] time: 0:17:23.738614 \n","[Epoch 1/1] [Batch 319/400] [D loss: 0.235265, acc:  67%] [G loss: 7.005653, adv: 0.594958, recon: 0.264725, id: 0.286150] time: 0:17:26.368715 \n","[Epoch 1/1] [Batch 320/400] [D loss: 0.183747, acc:  74%] [G loss: 8.570427, adv: 0.778188, recon: 0.319038, id: 0.380055] time: 0:17:28.981461 \n","[Epoch 1/1] [Batch 321/400] [D loss: 0.306329, acc:  69%] [G loss: 8.297363, adv: 0.650949, recon: 0.318435, id: 0.257680] time: 0:17:31.051660 \n","[Epoch 1/1] [Batch 322/400] [D loss: 0.332946, acc:  47%] [G loss: 7.400380, adv: 0.482455, recon: 0.294254, id: 0.272205] time: 0:17:34.033898 \n","[Epoch 1/1] [Batch 323/400] [D loss: 0.337924, acc:  48%] [G loss: 5.859841, adv: 0.471947, recon: 0.223801, id: 0.200166] time: 0:17:36.202525 \n","[Epoch 1/1] [Batch 324/400] [D loss: 0.161111, acc:  77%] [G loss: 9.448269, adv: 0.709940, recon: 0.365970, id: 0.351973] time: 0:17:38.929467 \n","[Epoch 1/1] [Batch 325/400] [D loss: 0.233770, acc:  62%] [G loss: 6.980608, adv: 0.621446, recon: 0.261680, id: 0.255013] time: 0:17:41.730697 \n","[Epoch 1/1] [Batch 326/400] [D loss: 0.320451, acc:  51%] [G loss: 7.125900, adv: 0.569180, recon: 0.272357, id: 0.236232] time: 0:17:44.821848 \n","[Epoch 1/1] [Batch 327/400] [D loss: 0.303413, acc:  53%] [G loss: 6.614785, adv: 0.607669, recon: 0.246072, id: 0.213783] time: 0:17:47.567302 \n","[Epoch 1/1] [Batch 328/400] [D loss: 0.215964, acc:  67%] [G loss: 8.929434, adv: 0.685490, recon: 0.344182, id: 0.327877] time: 0:17:50.205600 \n","[Epoch 1/1] [Batch 329/400] [D loss: 0.366735, acc:  44%] [G loss: 7.530389, adv: 0.553588, recon: 0.291059, id: 0.328643] time: 0:17:52.777112 \n","[Epoch 1/1] [Batch 330/400] [D loss: 0.472913, acc:  25%] [G loss: 8.138803, adv: 0.416827, recon: 0.332677, id: 0.309091] time: 0:17:56.084285 \n","[Epoch 1/1] [Batch 331/400] [D loss: 0.271645, acc:  54%] [G loss: 8.785951, adv: 0.529532, recon: 0.351222, id: 0.331691] time: 0:17:58.098375 \n","[Epoch 1/1] [Batch 332/400] [D loss: 0.299946, acc:  57%] [G loss: 7.865461, adv: 0.549560, recon: 0.311192, id: 0.274130] time: 0:18:01.073931 \n","[Epoch 1/1] [Batch 333/400] [D loss: 0.300987, acc:  52%] [G loss: 6.742623, adv: 0.532023, recon: 0.258395, id: 0.263336] time: 0:18:05.439113 \n","[Epoch 1/1] [Batch 334/400] [D loss: 0.218357, acc:  66%] [G loss: 9.695145, adv: 0.713760, recon: 0.376303, id: 0.397875] time: 0:18:08.792112 \n","[Epoch 1/1] [Batch 335/400] [D loss: 0.280966, acc:  60%] [G loss: 7.853533, adv: 0.605614, recon: 0.305068, id: 0.205844] time: 0:18:11.648037 \n","[Epoch 1/1] [Batch 336/400] [D loss: 0.325939, acc:  51%] [G loss: 9.344931, adv: 0.498383, recon: 0.380070, id: 0.310441] time: 0:18:15.177734 \n","[Epoch 1/1] [Batch 337/400] [D loss: 0.275860, acc:  56%] [G loss: 6.670894, adv: 0.637936, recon: 0.247351, id: 0.222530] time: 0:18:18.280781 \n","[Epoch 1/1] [Batch 338/400] [D loss: 0.224028, acc:  66%] [G loss: 7.594368, adv: 0.655279, recon: 0.284952, id: 0.338756] time: 0:18:20.761917 \n","[Epoch 1/1] [Batch 339/400] [D loss: 0.139674, acc:  82%] [G loss: 8.283497, adv: 0.770217, recon: 0.307265, id: 0.279476] time: 0:18:23.223281 \n","[Epoch 1/1] [Batch 340/400] [D loss: 0.274807, acc:  61%] [G loss: 8.048798, adv: 0.536145, recon: 0.316419, id: 0.270855] time: 0:18:27.585743 \n","[Epoch 1/1] [Batch 341/400] [D loss: 0.292870, acc:  57%] [G loss: 6.774665, adv: 0.565028, recon: 0.256497, id: 0.233200] time: 0:18:29.684007 \n","[Epoch 1/1] [Batch 342/400] [D loss: 0.247558, acc:  67%] [G loss: 9.614521, adv: 0.695458, recon: 0.374515, id: 0.468051] time: 0:18:31.815108 \n","[Epoch 1/1] [Batch 343/400] [D loss: 0.206847, acc:  68%] [G loss: 8.921347, adv: 0.640335, recon: 0.347923, id: 0.314752] time: 0:18:33.854213 \n","[Epoch 1/1] [Batch 344/400] [D loss: 0.433046, acc:  38%] [G loss: 8.794691, adv: 0.530493, recon: 0.350909, id: 0.372117] time: 0:18:37.013884 \n","[Epoch 1/1] [Batch 345/400] [D loss: 0.159987, acc:  77%] [G loss: 7.284077, adv: 0.729158, recon: 0.264995, id: 0.212284] time: 0:18:40.245304 \n","[Epoch 1/1] [Batch 346/400] [D loss: 0.234448, acc:  65%] [G loss: 7.589102, adv: 0.610529, recon: 0.289871, id: 0.292957] time: 0:18:43.288692 \n","[Epoch 1/1] [Batch 347/400] [D loss: 0.422065, acc:  41%] [G loss: 7.658451, adv: 0.548811, recon: 0.298663, id: 0.346919] time: 0:18:45.893441 \n","[Epoch 1/1] [Batch 348/400] [D loss: 0.321939, acc:  47%] [G loss: 6.354815, adv: 0.543299, recon: 0.240891, id: 0.218895] time: 0:18:49.371325 \n","[Epoch 1/1] [Batch 349/400] [D loss: 0.344623, acc:  49%] [G loss: 5.969318, adv: 0.529658, recon: 0.223322, id: 0.240299] time: 0:18:51.469127 \n","[Epoch 1/1] [Batch 350/400] [D loss: 0.290161, acc:  53%] [G loss: 10.160788, adv: 0.584235, recon: 0.408818, id: 0.275271] time: 0:18:54.610709 \n","[Epoch 1/1] [Batch 351/400] [D loss: 0.162785, acc:  75%] [G loss: 9.876331, adv: 0.790865, recon: 0.379426, id: 0.274037] time: 0:18:57.308966 \n","[Epoch 1/1] [Batch 352/400] [D loss: 0.399264, acc:  38%] [G loss: 7.127049, adv: 0.554022, recon: 0.273865, id: 0.190929] time: 0:18:59.439735 \n","[Epoch 1/1] [Batch 353/400] [D loss: 0.224085, acc:  69%] [G loss: 9.107830, adv: 0.645214, recon: 0.355553, id: 0.219608] time: 0:19:02.646700 \n","[Epoch 1/1] [Batch 354/400] [D loss: 0.352699, acc:  45%] [G loss: 7.657164, adv: 0.570428, recon: 0.297459, id: 0.212551] time: 0:19:05.613203 \n","[Epoch 1/1] [Batch 355/400] [D loss: 0.257373, acc:  62%] [G loss: 8.255770, adv: 0.664769, recon: 0.316200, id: 0.322458] time: 0:19:08.218505 \n","[Epoch 1/1] [Batch 356/400] [D loss: 0.105715, acc:  88%] [G loss: 8.460812, adv: 0.881620, recon: 0.304181, id: 0.284035] time: 0:19:10.215963 \n","[Epoch 1/1] [Batch 357/400] [D loss: 0.239811, acc:  63%] [G loss: 7.804193, adv: 0.650747, recon: 0.295169, id: 0.302434] time: 0:19:13.577177 \n","[Epoch 1/1] [Batch 358/400] [D loss: 0.563572, acc:  24%] [G loss: 7.207722, adv: 0.419892, recon: 0.291191, id: 0.213314] time: 0:19:16.798257 \n","[Epoch 1/1] [Batch 359/400] [D loss: 0.164904, acc:  75%] [G loss: 7.411386, adv: 0.664701, recon: 0.277568, id: 0.255252] time: 0:19:18.994199 \n","[Epoch 1/1] [Batch 360/400] [D loss: 0.209524, acc:  66%] [G loss: 7.935664, adv: 0.697523, recon: 0.295756, id: 0.350391] time: 0:19:22.322461 \n","[Epoch 1/1] [Batch 361/400] [D loss: 0.349906, acc:  51%] [G loss: 7.183373, adv: 0.652285, recon: 0.265959, id: 0.258536] time: 0:19:25.382143 \n","[Epoch 1/1] [Batch 362/400] [D loss: 0.243460, acc:  65%] [G loss: 9.780085, adv: 0.638318, recon: 0.387007, id: 0.353250] time: 0:19:27.992444 \n","[Epoch 1/1] [Batch 363/400] [D loss: 0.199796, acc:  69%] [G loss: 8.891180, adv: 0.602685, recon: 0.349924, id: 0.334171] time: 0:19:30.135416 \n","[Epoch 1/1] [Batch 364/400] [D loss: 0.329511, acc:  49%] [G loss: 7.215455, adv: 0.540031, recon: 0.278043, id: 0.264552] time: 0:19:33.413425 \n","[Epoch 1/1] [Batch 365/400] [D loss: 0.297148, acc:  57%] [G loss: 7.559940, adv: 0.608914, recon: 0.287790, id: 0.258435] time: 0:19:35.658318 \n","[Epoch 1/1] [Batch 366/400] [D loss: 0.401700, acc:  42%] [G loss: 7.290157, adv: 0.561933, recon: 0.279836, id: 0.304118] time: 0:19:38.756845 \n","[Epoch 1/1] [Batch 367/400] [D loss: 0.187363, acc:  75%] [G loss: 9.753071, adv: 0.723543, recon: 0.377018, id: 0.384456] time: 0:19:40.875678 \n","[Epoch 1/1] [Batch 368/400] [D loss: 0.427911, acc:  38%] [G loss: 6.752359, adv: 0.462680, recon: 0.264849, id: 0.270212] time: 0:19:43.830062 \n","[Epoch 1/1] [Batch 369/400] [D loss: 0.266677, acc:  55%] [G loss: 7.761146, adv: 0.611458, recon: 0.294989, id: 0.309863] time: 0:19:46.090459 \n","[Epoch 1/1] [Batch 370/400] [D loss: 0.218905, acc:  67%] [G loss: 8.348992, adv: 0.615712, recon: 0.324127, id: 0.371224] time: 0:19:48.967707 \n","[Epoch 1/1] [Batch 371/400] [D loss: 0.158014, acc:  77%] [G loss: 7.496246, adv: 0.743014, recon: 0.270958, id: 0.300298] time: 0:19:51.049391 \n","[Epoch 1/1] [Batch 372/400] [D loss: 0.231459, acc:  64%] [G loss: 6.377955, adv: 0.611629, recon: 0.232613, id: 0.249049] time: 0:19:54.416757 \n","[Epoch 1/1] [Batch 373/400] [D loss: 0.094614, acc:  92%] [G loss: 8.635383, adv: 0.777804, recon: 0.321925, id: 0.289168] time: 0:19:57.264886 \n","[Epoch 1/1] [Batch 374/400] [D loss: 0.282665, acc:  63%] [G loss: 8.357231, adv: 0.821668, recon: 0.306214, id: 0.357584] time: 0:19:59.250816 \n","[Epoch 1/1] [Batch 375/400] [D loss: 0.433028, acc:  48%] [G loss: 8.980139, adv: 0.677998, recon: 0.343501, id: 0.356251] time: 0:20:01.463442 \n","[Epoch 1/1] [Batch 376/400] [D loss: 0.564776, acc:  30%] [G loss: 8.260607, adv: 0.527548, recon: 0.325346, id: 0.368862] time: 0:20:04.219767 \n","[Epoch 1/1] [Batch 377/400] [D loss: 0.356878, acc:  49%] [G loss: 8.217076, adv: 0.649873, recon: 0.313534, id: 0.308970] time: 0:20:07.225615 \n","[Epoch 1/1] [Batch 378/400] [D loss: 0.323824, acc:  52%] [G loss: 6.507914, adv: 0.580495, recon: 0.242149, id: 0.260346] time: 0:20:10.397106 \n","[Epoch 1/1] [Batch 379/400] [D loss: 0.355336, acc:  45%] [G loss: 7.709013, adv: 0.484259, recon: 0.307719, id: 0.266509] time: 0:20:13.054313 \n","[Epoch 1/1] [Batch 380/400] [D loss: 0.333907, acc:  53%] [G loss: 6.280903, adv: 0.534160, recon: 0.237277, id: 0.214128] time: 0:20:15.862052 \n","[Epoch 1/1] [Batch 381/400] [D loss: 0.226577, acc:  66%] [G loss: 7.268898, adv: 0.696083, recon: 0.267366, id: 0.272310] time: 0:20:18.520875 \n","[Epoch 1/1] [Batch 382/400] [D loss: 0.419022, acc:  53%] [G loss: 8.532472, adv: 0.622854, recon: 0.329924, id: 0.329082] time: 0:20:21.236531 \n","[Epoch 1/1] [Batch 383/400] [D loss: 0.420992, acc:  37%] [G loss: 6.309619, adv: 0.545674, recon: 0.236824, id: 0.200341] time: 0:20:24.090008 \n","[Epoch 1/1] [Batch 384/400] [D loss: 0.238893, acc:  61%] [G loss: 6.687399, adv: 0.596728, recon: 0.248882, id: 0.262592] time: 0:20:26.227861 \n","[Epoch 1/1] [Batch 385/400] [D loss: 0.229838, acc:  64%] [G loss: 7.044354, adv: 0.617114, recon: 0.264814, id: 0.244240] time: 0:20:30.032398 \n","[Epoch 1/1] [Batch 386/400] [D loss: 0.353397, acc:  48%] [G loss: 7.529254, adv: 0.528695, recon: 0.294519, id: 0.312258] time: 0:20:32.464836 \n","[Epoch 1/1] [Batch 387/400] [D loss: 0.312231, acc:  60%] [G loss: 6.918005, adv: 0.641097, recon: 0.256095, id: 0.245297] time: 0:20:34.775040 \n","[Epoch 1/1] [Batch 388/400] [D loss: 0.494111, acc:  51%] [G loss: 7.625375, adv: 0.573485, recon: 0.294174, id: 0.283604] time: 0:20:38.356563 \n","[Epoch 1/1] [Batch 389/400] [D loss: 0.258725, acc:  64%] [G loss: 7.282828, adv: 0.640546, recon: 0.271338, id: 0.283131] time: 0:20:41.040059 \n","[Epoch 1/1] [Batch 390/400] [D loss: 0.342946, acc:  42%] [G loss: 6.590658, adv: 0.526470, recon: 0.249581, id: 0.235013] time: 0:20:43.169252 \n","[Epoch 1/1] [Batch 391/400] [D loss: 0.469579, acc:  33%] [G loss: 6.873480, adv: 0.451897, recon: 0.272512, id: 0.214479] time: 0:20:45.835718 \n","[Epoch 1/1] [Batch 392/400] [D loss: 0.333590, acc:  46%] [G loss: 5.527999, adv: 0.555848, recon: 0.199167, id: 0.178564] time: 0:20:48.062028 \n","[Epoch 1/1] [Batch 393/400] [D loss: 0.342534, acc:  44%] [G loss: 6.564795, adv: 0.532685, recon: 0.248942, id: 0.242457] time: 0:20:51.028766 \n","[Epoch 1/1] [Batch 394/400] [D loss: 0.454483, acc:  36%] [G loss: 6.968015, adv: 0.521470, recon: 0.268463, id: 0.248022] time: 0:20:53.438334 \n","[Epoch 1/1] [Batch 395/400] [D loss: 0.347610, acc:  45%] [G loss: 8.410359, adv: 0.552215, recon: 0.331824, id: 0.380538] time: 0:20:56.613116 \n","[Epoch 1/1] [Batch 396/400] [D loss: 0.252612, acc:  57%] [G loss: 6.948480, adv: 0.601574, recon: 0.259982, id: 0.309359] time: 0:20:59.626135 \n","[Epoch 1/1] [Batch 397/400] [D loss: 0.252290, acc:  58%] [G loss: 7.398881, adv: 0.639082, recon: 0.276116, id: 0.352328] time: 0:21:02.206233 \n","[Epoch 1/1] [Batch 398/400] [D loss: 0.378854, acc:  35%] [G loss: 6.296754, adv: 0.508377, recon: 0.238920, id: 0.217719] time: 0:21:05.327142 \n","[Epoch 1/1] [Batch 399/400] [D loss: 0.282424, acc:  54%] [G loss: 7.505562, adv: 0.580338, recon: 0.289053, id: 0.324326] time: 0:21:07.999895 \n","[Epoch 1/1] [Batch 400/400] [D loss: 0.422428, acc:  49%] [G loss: 9.125837, adv: 0.544548, recon: 0.365514, id: 0.429995] time: 0:21:10.418506 \n","2020-09-04T05:24:04.100600 End\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IUoanYQ_fvgU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1599197106526,"user_tz":-540,"elapsed":2370,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}},"outputId":"80cf0928-698f-4e91-a085-ecdc1c18c3ff"},"source":["gan.save_models()"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Model weights saved. combined_model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZWdJbZTik0e5","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}